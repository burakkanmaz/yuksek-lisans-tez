Senaryo KodlarÄ±
ğŸ§ª Senaryo 1: Bir dosya yÃ¼kleme sistemi kullanÄ±cÄ±larÄ±n bÃ¼yÃ¼k dosyalar yÃ¼klemesine imkan tanÄ±yor. Sistem yÃ¼klenen dosyalarÄ± geÃ§ici olarak sunucu diskinde saklar. KullanÄ±cÄ±lar farklÄ± boyutlarda dosyalar yÃ¼kleyebilir. YÃ¼kleme iÅŸlemi sÄ±rasÄ±nda disk alanÄ± kullanÄ±lÄ±r.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
[HttpPost("upload")]
public async Task<IActionResult> UploadFile(IFormFile file)
{
    if (file == null || file.Length == 0)
        return BadRequest("No file uploaded.");

    var tempPath = Path.Combine(Path.GetTempPath(), Guid.NewGuid().ToString() + Path.GetExtension(file.FileName));
    using (var stream = new FileStream(tempPath, FileMode.Create))
    {
        await file.CopyToAsync(stream);
    }

    return Ok(new { FilePath = tempPath });
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from fastapi import FastAPI, File, UploadFile
import shutil
import tempfile
import os

app = FastAPI()

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    if not file:
        return {"error": "No file uploaded"}
    
    temp_file = os.path.join(tempfile.gettempdir(), f"{file.filename}")
    with open(temp_file, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    return {"file_path": temp_file}


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import express from 'express';
import multer from 'multer';
import path from 'path';

const app = express();
const upload = multer({ dest: 'temp/' });

app.post('/upload', upload.single('file'), (req, res) => {
    if (!req.file) {
        return res.status(400).json({ error: 'No file uploaded' });
    }
    res.json({ filePath: path.join('temp', req.file.filename) });
});


ğŸ§ª Senaryo 2: Bir API servisi dakikada Ã§ok sayÄ±da istek alabiliyor. KullanÄ±cÄ±lar bu servise sÃ¼rekli Ã§aÄŸrÄ± yapabilir. Sistem gelen istekleri iÅŸleyerek yanÄ±t dÃ¶ndÃ¼rÃ¼r. Her istek sunucu kaynaklarÄ±nÄ± kullanÄ±r.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
[HttpGet("process")]
[Route("api/process")]
public IActionResult ProcessRequest([FromQuery] string data)
{
    if (string.IsNullOrEmpty(data))
        return BadRequest("Data is required.");

    // Simulate processing
    var result = $"Processed: {data.ToUpper()}";
    return Ok(new { Result = result });
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class RequestData(BaseModel):
    data: str

@app.get("/process")
async def process_request(data: str):
    if not data:
        return {"error": "Data is required"}
    
    result = f"Processed: {data.upper()}"
    return {"result": result}


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import express from 'express';

const app = express();
app.use(express.json());

app.get('/process', (req, res) => {
    const { data } = req.query;
    if (!data) {
        return res.status(400).json({ error: 'Data is required' });
    }
    const result = `Processed: ${(data as string).toUpperCase()}`;
    res.json({ result });
});


ğŸ§ª Senaryo 3: Bir log sistemi sÃ¼rekli olarak gelen verileri dosyalara yazÄ±yor. Uygulama Ã§alÄ±ÅŸÄ±rken farklÄ± modÃ¼llerden log mesajlarÄ± gelir. Bu mesajlar dosyalara sÄ±ralÄ± ÅŸekilde kaydedilir. Sistem Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼re boyunca log dosyalarÄ± bÃ¼yÃ¼r.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class Logger
{
    private readonly string _logFilePath = "logs/app.log";

    public void Log(string message)
    {
        var logEntry = $"{DateTime.UtcNow:yyyy-MM-dd HH:mm:ss} - {message}{Environment.NewLine}";
        File.AppendAllText(_logFilePath, logEntry);
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
import datetime
import os

class Logger:
    def __init__(self):
        self.log_file = "logs/app.log"
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)

    def log(self, message: str):
        log_entry = f"{datetime.datetime.utcnow():%Y-%m-%d %H:%M:%S} - {message}\n"
        with open(self.log_file, "a") as f:
            f.write(log_entry)


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import fs from 'fs';
import path from 'path';

class Logger {
    private logFilePath = path.join('logs', 'app.log');

    constructor() {
        fs.mkdirSync(path.dirname(this.logFilePath), { recursive: true });
    }

    log(message: string): void {
        const logEntry = `${new Date().toISOString()} - ${message}\n`;
        fs.appendFileSync(this.logFilePath, logEntry);
    }
}


ğŸ§ª Senaryo 4: Bir web uygulamasÄ±nda kullanÄ±cÄ±lar karmaÅŸÄ±k arama sorgularÄ± yapabiliyor. Arama iÅŸlemi veritabanÄ±nda birden fazla tablo Ã¼zerinde Ã§alÄ±ÅŸÄ±r. DetaylÄ± arama kriterleri Ã§ok sayÄ±da kayÄ±t kontrolÃ¼ gerektirir. Arama sonuÃ§larÄ± kullanÄ±cÄ±ya liste halinde sunulur.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
[HttpGet("search")]
public IActionResult Search([FromQuery] string keyword, [FromQuery] int? categoryId)
{
    var query = _dbContext.Products.AsQueryable();
    if (!string.IsNullOrEmpty(keyword))
        query = query.Where(p => p.Name.Contains(keyword));
    if (categoryId.HasValue)
        query = query.Where(p => p.CategoryId == categoryId);

    var results = query.Take(100).ToList();
    return Ok(results);
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from fastapi import FastAPI
from sqlalchemy import create_engine, select
from sqlalchemy.orm import Session

app = FastAPI()
engine = create_engine("sqlite:///app.db")

@app.get("/search")
async def search(keyword: str = None, category_id: int = None):
    with Session(engine) as session:
        query = select(Product)
        if keyword:
            query = query.filter(Product.name.contains(keyword))
        if category_id:
            query = query.filter(Product.category_id == category_id)
        
        results = session.exec(query.limit(100)).all()
        return results


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import express from 'express';
import { Pool } from 'pg';

const app = express();
const pool = new Pool({ connectionString: 'postgresql://user:pass@localhost/db' });

app.get('/search', async (req, res) => {
    const { keyword, categoryId } = req.query;
    let query = 'SELECT * FROM products WHERE 1=1';
    const values: any[] = [];

    if (keyword) {
        query += ' AND name ILIKE $1';
        values.push(`%${keyword}%`);
    }
    if (categoryId) {
        query += ' AND category_id = $2';
        values.push(categoryId);
    }

    const results = await pool.query(query + ' LIMIT 100', values);
    res.json(results.rows);
});


ğŸ§ª Senaryo 5: Bir video iÅŸleme uygulamasÄ± kullanÄ±cÄ±larÄ±n yÃ¼klediÄŸi videolarÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor. FarklÄ± formatlardaki videolar sisteme yÃ¼klenebilir. DÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi sÄ±rasÄ±nda Ã§eÅŸitli kodekler kullanÄ±lÄ±r. Ä°ÅŸlenmiÅŸ videolar kullanÄ±cÄ±lara sunulur.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class VideoProcessor
{
    public async Task<string> ConvertVideo(string inputPath, string outputFormat)
    {
        var outputPath = Path.ChangeExtension(inputPath, outputFormat);
        var process = new Process
        {
            StartInfo = new ProcessStartInfo
            {
                FileName = "ffmpeg",
                Arguments = $"-i \"{inputPath}\" \"{outputPath}\"",
                RedirectStandardOutput = true,
                UseShellExecute = false
            }
        };

        process.Start();
        await process.WaitForExitAsync();
        return outputPath;
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
import subprocess
import os

class VideoProcessor:
    def convert_video(self, input_path: str, output_format: str) -> str:
        output_path = os.path.splitext(input_path)[0] + f".{output_format}"
        cmd = ["ffmpeg", "-i", input_path, output_path]
        subprocess.run(cmd, check=True)
        return output_path


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import { exec } from 'child_process';
import { promisify } from 'util';
import path from 'path';

const execAsync = promisify(exec);

class VideoProcessor {
    async convertVideo(inputPath: string, outputFormat: string): Promise<string> {
        const outputPath = path.join(path.dirname(inputPath), `${path.basename(inputPath, path.extname(inputPath))}.${outputFormat}`);
        await execAsync(`ffmpeg -i "${inputPath}" "${outputPath}"`);
        return outputPath;
    }
}


ğŸ§ª Senaryo 6: Bir sosyal medya platformu kullanÄ±cÄ±larÄ±n fotoÄŸraflarÄ±nÄ± yÃ¼klemelerine olanak tanÄ±yor. YÃ¼klenen fotoÄŸraflar farklÄ± boyutlarda olabilir. Sistem bu fotoÄŸraflarÄ± thumbnail oluÅŸturmak iÃ§in iÅŸler. Ä°ÅŸlenen fotoÄŸraflar kullanÄ±cÄ± profillerinde gÃ¶sterilir.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class ImageProcessor
{
    public void CreateThumbnail(string inputPath, string outputPath, int width)
    {
        using (var image = Image.Load(inputPath))
        {
            image.Mutate(x => x.Resize(new ResizeOptions
            {
                Size = new Size(width, 0),
                Mode = ResizeMode.Max
            }));
            image.Save(outputPath);
        }
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from PIL import Image

class ImageProcessor:
    def create_thumbnail(self, input_path: str, output_path: str, width: int):
        with Image.open(input_path) as img:
            img.thumbnail((width, width))
            img.save(output_path)


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import sharp from 'sharp';

class ImageProcessor {
    async createThumbnail(inputPath: string, outputPath: string, width: number): Promise<void> {
        await sharp(inputPath)
            .resize({ width, fit: 'inside' })
            .toFile(outputPath);
    }
}


ğŸ§ª Senaryo 7: Bir oyun sunucusu aynÄ± anda Ã§ok sayÄ±da oyuncuya hizmet verebiliyor. Her oyuncu sunucuyla aktif baÄŸlantÄ± kurar. Oyun sÄ±rasÄ±nda oyuncular arasÄ±nda sÃ¼rekli veri alÄ±ÅŸveriÅŸi yapÄ±lÄ±r. Sunucu tÃ¼m oyuncu verilerini bellekte tutar.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class GameServer
{
    private readonly ConcurrentDictionary<string, Player> _players = new();

    public void HandlePlayerData(string playerId, PlayerData data)
    {
        _players.AddOrUpdate(playerId, new Player(data), (key, old) => old.Update(data));
        BroadcastToPlayers(playerId, data);
    }

    private void BroadcastToPlayers(string senderId, PlayerData data)
    {
        // Simulate broadcasting
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from collections import defaultdict
from typing import Dict

class GameServer:
    def __init__(self):
        self.players: Dict[str, Player] = defaultdict(Player)

    def handle_player_data(self, player_id: str, data: dict):
        self.players[player_id].update(data)
        self._broadcast_to_players(player_id, data)

    def _broadcast_to_players(self, sender_id: str, data: dict):
        # Simulate broadcasting
        pass


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
interface PlayerData {
    id: string;
    position: { x: number; y: number };
}

class GameServer {
    private players = new Map<string, PlayerData>();

    handlePlayerData(playerId: string, data: PlayerData): void {
        this.players.set(playerId, data);
        this.broadcastToPlayers(playerId, data);
    }

    private broadcastToPlayers(senderId: string, data: PlayerData): void {
        // Simulate broadcasting
    }
}


ğŸ§ª Senaryo 8: Bir mail sistemi kullanÄ±cÄ±larÄ±n bÃ¼yÃ¼k ekler gÃ¶ndermesine izin veriyor. E-postalar ek dosyalarÄ±yla birlikte sunucuda saklanÄ±r. KullanÄ±cÄ±lar Ã§eÅŸitli tÃ¼rde dosyalar ekleyebilir. Mail kutularÄ± zaman iÃ§inde bÃ¼yÃ¼r.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class MailService
{
    public async Task SaveEmail(Email email, IFormFile attachment)
    {
        var emailEntity = new EmailEntity { Subject = email.Subject, Body = email.Body };
        _dbContext.Emails.Add(emailEntity);

        if (attachment != null)
        {
            var filePath = Path.Combine("attachments", Guid.NewGuid().ToString());
            using (var stream = new FileStream(filePath, FileMode.Create))
            {
                await attachment.CopyToAsync(stream);
            }
            emailEntity.AttachmentPath = filePath;
        }

        await _dbContext.SaveChangesAsync();
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from fastapi import FastAPI, File, UploadFile
from sqlalchemy.orm import Session

app = FastAPI()

async def save_email(db: Session, email: Email, attachment: UploadFile = None):
    email_entity = EmailEntity(subject=email.subject, body=email.body)
    db.add(email_entity)

    if attachment:
        file_path = f"attachments/{uuid.uuid4()}"
        with open(file_path, "wb") as f:
            shutil.copyfileobj(attachment.file, f)
        email_entity.attachment_path = file_path

    db.commit()


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import express from 'express';
import multer from 'multer';
import { Pool } from 'pg';

const app = express();
const upload = multer({ dest: 'attachments/' });
const pool = new Pool({ connectionString: 'postgresql://user:pass@localhost/db' });

app.post('/email', upload.single('attachment'), async (req, res) => {
    const { subject, body } = req.body;
    const attachmentPath = req.file ? req.file.path : null;

    await pool.query(
        'INSERT INTO emails (subject, body, attachment_path) VALUES ($1, $2, $3)',
        [subject, body, attachmentPath]
    );
    res.json({ success: true });
});


ğŸ§ª Senaryo 9: Bir raporlama sistemi bÃ¼yÃ¼k veri setleri Ã¼zerinde analiz yapÄ±yor. KullanÄ±cÄ±lar geniÅŸ tarih aralÄ±klarÄ±nda raporlar oluÅŸturabilir. Sistem milyonlarca kayÄ±t Ã¼zerinde hesaplama yapar. OluÅŸturulan raporlar kullanÄ±cÄ±lara sunulur.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class ReportService
{
    public async Task<Report> GenerateReport(DateTime startDate, DateTime endDate)
    {
        var query = _dbContext.Transactions
            .Where(t => t.Date >= startDate && t.Date <= endDate)
            .GroupBy(t => t.Category)
            .Select(g => new { Category = g.Key, Total = g.Sum(t => t.Amount) });

        var results = await query.ToListAsync();
        return new Report { Data = results };
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
from sqlalchemy import create_engine, select
from sqlalchemy.orm import Session
from datetime import datetime

engine = create_engine("sqlite:///app.db")

async def generate_report(start_date: datetime, end_date: datetime):
    with Session(engine) as session:
        query = select(Transaction.category, func.sum(Transaction.amount).label("total")) \
            .filter(Transaction.date.between(start_date, end_date)) \
            .group_by(Transaction.category)
        
        results = session.exec(query).all()
        return {"data": results}


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import { Pool } from 'pg';

const pool = new Pool({ connectionString: 'postgresql://user:pass@localhost/db' });

async function generateReport(startDate: Date, endDate: Date): Promise<any> {
    const query = `
        SELECT category, SUM(amount) as total
        FROM transactions
        WHERE date BETWEEN $1 AND $2
        GROUP BY category
    `;
    const results = await pool.query(query, [startDate, endDate]);
    return { data: results.rows };
}


ğŸ§ª Senaryo 10: Bir backup sistemi kullanÄ±cÄ± verilerini dÃ¼zenli olarak yedekliyor. FarklÄ± boyutlardaki dosyalar yedekleme sÃ¼recine dahil edilir. Sistem tÃ¼m kullanÄ±cÄ± verilerini r ve arÅŸivler. Yedekleme iÅŸlemi otomatik olarak Ã§alÄ±ÅŸÄ±r.
ğŸ’» Dil: C#ğŸ¤– AI: Grok  
public class BackupService
{
    public void BackupUserData(string sourcePath, string backupPath)
    {
        var archivePath = Path.Combine(backupPath, $"backup_{DateTime.UtcNow:yyyyMMdd}.zip");
        Directory.CreateDirectory(backupPath);
        ZipFile.CreateFromDirectory(sourcePath, archivePath);
    }
}


ğŸ’» Dil: PythonğŸ¤– AI: Grok  
import shutil
import os
from datetime import datetime

class BackupService:
    def backup_user_data(self, source_path: str, backup_path: str):
        os.makedirs(backup_path, exist_ok=True)
        archive_name = f"backup_{datetime.utcnow():%Y%m%d}"
        shutil.make_archive(
            os.path.join(backup_path, archive_name),
            "zip",
            source_path
        )


ğŸ’» Dil: TypeScriptğŸ¤– AI: Grok  
import fs from 'fs';
import path from 'path';
import archiver from 'archiver';

class BackupService {
    backupUserData(sourcePath: string, backupPath: string): void {
        fs.mkdirSync(backupPath, { recursive: true });
        const archivePath = path.join(backupPath, `backup_${new Date().toISOString().slice(0, 10).replace(/-/g, '')}.zip`);
        const output = fs.createWriteStream(archivePath);
        const archive = archiver('zip');
        archive.pipe(output);
        archive.directory(sourcePath, false);
        archive.finalize();
    }
}

