### ðŸ§ª Senaryo 1: Bir dosya yÃ¼kleme sistemi kullanÄ±cÄ±larÄ±n bÃ¼yÃ¼k dosyalar yÃ¼klemesine imkan tanÄ±yor. Sistem yÃ¼klenen dosyalarÄ± geÃ§ici olarak sunucu diskinde saklar. KullanÄ±cÄ±lar farklÄ± boyutlarda dosyalar yÃ¼kleyebilir. YÃ¼kleme iÅŸlemi sÄ±rasÄ±nda disk alanÄ± kullanÄ±lÄ±r.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 29
```csharp
[HttpPost("upload")]
public async Task<IActionResult> UploadFile(IFormFile file)
{
    if (file == null || file.Length == 0)
        return BadRequest("No file uploaded");

    var tempPath = Path.GetTempPath();
    var fileName = Guid.NewGuid().ToString() + Path.GetExtension(file.FileName);
    var filePath = Path.Combine(tempPath, fileName);

    using (var stream = new FileStream(filePath, FileMode.Create))
    {
        await file.CopyToAsync(stream);
    }

    var fileInfo = new FileUpload
    {
        Id = Guid.NewGuid(),
        OriginalName = file.FileName,
        TempPath = filePath,
        Size = file.Length,
        UploadedAt = DateTime.UtcNow
    };

    _context.FileUploads.Add(fileInfo);
    await _context.SaveChangesAsync();

    return Ok(new { FileId = fileInfo.Id, Size = file.Length });
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 37
```python
import os
import uuid
import tempfile
from fastapi import FastAPI, UploadFile, HTTPException
from sqlalchemy.orm import Session
from models import FileUpload, get_db

app = FastAPI()

@app.post("/upload")
async def upload_file(file: UploadFile, db: Session = Depends(get_db)):
    if not file.filename:
        raise HTTPException(status_code=400, detail="No file uploaded")

    temp_dir = tempfile.gettempdir()
    file_id = str(uuid.uuid4())
    file_extension = os.path.splitext(file.filename)[1]
    temp_filename = f"{file_id}{file_extension}"
    file_path = os.path.join(temp_dir, temp_filename)

    with open(file_path, "wb") as buffer:
        content = await file.read()
        buffer.write(content)
        file_size = len(content)

    file_upload = FileUpload(
        id=file_id,
        original_name=file.filename,
        temp_path=file_path,
        size=file_size,
        uploaded_at=datetime.utcnow()
    )

    db.add(file_upload)
    db.commit()

    return {"file_id": file_id, "size": file_size}
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 42
```typescript
import express from 'express';
import multer from 'multer';
import path from 'path';
import os from 'os';
import { v4 as uuidv4 } from 'uuid';
import { FileUpload } from './models/FileUpload';

const app = express();

const storage = multer.diskStorage({
    destination: (req, file, cb) => {
        cb(null, os.tmpdir());
    },
    filename: (req, file, cb) => {
        const fileId = uuidv4();
        const ext = path.extname(file.originalname);
        cb(null, `${fileId}${ext}`);
    }
});

const upload = multer({ storage });

app.post('/upload', upload.single('file'), async (req, res) => {
    if (!req.file) {
        return res.status(400).json({ error: 'No file uploaded' });
    }

    const fileUpload = new FileUpload({
        id: uuidv4(),
        originalName: req.file.originalname,
        tempPath: req.file.path,
        size: req.file.size,
        uploadedAt: new Date()
    });

    await fileUpload.save();

    res.json({
        fileId: fileUpload.id,
        size: req.file.size
    });
});
```

### ðŸ§ª Senaryo 2: Bir API servisi dakikada Ã§ok sayÄ±da istek alabiliyor. KullanÄ±cÄ±lar bu servise sÃ¼rekli Ã§aÄŸrÄ± yapabilir. Sistem gelen istekleri iÅŸleyerek yanÄ±t dÃ¶ndÃ¼rÃ¼r. Her istek sunucu kaynaklarÄ±nÄ± kullanÄ±r.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 49
```csharp
[ApiController]
[Route("api/[controller]")]
public class DataController : ControllerBase
{
    private readonly IMemoryCache _cache;
    private readonly ILogger<DataController> _logger;

    public DataController(IMemoryCache cache, ILogger<DataController> logger)
    {
        _cache = cache;
        _logger = logger;
    }

    [HttpGet("process")]
    [EnableRateLimiting("ApiPolicy")]
    public async Task<IActionResult> ProcessRequest([FromQuery] string query)
    {
        var requestId = Guid.NewGuid().ToString();
        _logger.LogInformation($"Processing request {requestId} with query: {query}");

        var cacheKey = $"result_{query?.GetHashCode()}";

        if (_cache.TryGetValue(cacheKey, out var cachedResult))
        {
            return Ok(new { RequestId = requestId, Data = cachedResult, FromCache = true });
        }

        // Simulate processing
        await Task.Delay(Random.Shared.Next(100, 500));

        var result = new
        {
            ProcessedAt = DateTime.UtcNow,
            Query = query,
            Results = GenerateResults(query)
        };

        _cache.Set(cacheKey, result, TimeSpan.FromMinutes(5));

        return Ok(new { RequestId = requestId, Data = result, FromCache = false });
    }

    private object[] GenerateResults(string query)
    {
        return Enumerable.Range(1, Random.Shared.Next(10, 100))
            .Select(i => new { Id = i, Value = $"Result_{i}_{query}" })
            .ToArray();
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 54
```python
import asyncio
import time
import uuid
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import redis
import json

app = FastAPI()
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

redis_client = redis.Redis(host='localhost', port=6379, db=0)

@app.get("/api/process")
@limiter.limit("60/minute")
async def process_request(request: Request, query: str = None):
    request_id = str(uuid.uuid4())

    # Check cache
    cache_key = f"result_{hash(query) if query else 'empty'}"
    cached_result = redis_client.get(cache_key)

    if cached_result:
        return {
            "request_id": request_id,
            "data": json.loads(cached_result),
            "from_cache": True
        }

    # Simulate processing
    await asyncio.sleep(random.uniform(0.1, 0.5))

    result = {
        "processed_at": time.time(),
        "query": query,
        "results": [
            {"id": i, "value": f"Result_{i}_{query}"}
            for i in range(1, random.randint(10, 100))
        ]
    }

    # Cache result for 5 minutes
    redis_client.setex(cache_key, 300, json.dumps(result))

    return {
        "request_id": request_id,
        "data": result,
        "from_cache": False
    }
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 80
```typescript
import express from 'express';
import rateLimit from 'express-rate-limit';
import NodeCache from 'node-cache';
import { v4 as uuidv4 } from 'uuid';

const app = express();
const cache = new NodeCache({ stdTTL: 300 }); // 5 minutes

const limiter = rateLimit({
    windowMs: 60 * 1000, // 1 minute
    max: 60, // limit each IP to 60 requests per windowMs
    message: 'Too many requests from this IP'
});

app.use('/api', limiter);

interface ProcessResult {
    processedAt: number;
    query: string;
    results: Array<{ id: number; value: string }>;
}

app.get('/api/process', async (req, res) => {
    const requestId = uuidv4();
    const query = req.query.query as string;

    console.log(`Processing request ${requestId} with query: ${query}`);

    const cacheKey = `result_${query?.hashCode() || 'empty'}`;
    const cachedResult = cache.get<ProcessResult>(cacheKey);

    if (cachedResult) {
        return res.json({
            requestId,
            data: cachedResult,
            fromCache: true
        });
    }

    // Simulate processing delay
    await new Promise(resolve =>
        setTimeout(resolve, Math.random() * 400 + 100)
    );

    const result: ProcessResult = {
        processedAt: Date.now(),
        query,
        results: Array.from({ length: Math.floor(Math.random() * 90) + 10 },
            (_, i) => ({
                id: i + 1,
                value: `Result_${i + 1}_${query}`
            })
        )
    };

    cache.set(cacheKey, result);

    res.json({
        requestId,
        data: result,
        fromCache: false
    });
});

// Helper for string hashing
declare global {
    interface String {
        hashCode(): number;
    }
}

String.prototype.hashCode = function() {
    let hash = 0;
    for (let i = 0; i < this.length; i++) {
        const char = this.charCodeAt(i);
        hash = ((hash << 5) - hash) + char;
        hash = hash & hash;
    }
    return hash;
};
```

### ðŸ§ª Senaryo 3: Bir log sistemi sÃ¼rekli olarak gelen verileri dosyalara yazÄ±yor. Uygulama Ã§alÄ±ÅŸÄ±rken farklÄ± modÃ¼llerden log mesajlarÄ± gelir. Bu mesajlar dosyalara sÄ±ralÄ± ÅŸekilde kaydedilir. Sistem Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼re boyunca log dosyalarÄ± bÃ¼yÃ¼r.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 90
```csharp
public class LogManager
{
    private readonly object _lockObject = new object();
    private readonly string _logDirectory;
    private readonly Queue<LogEntry> _logQueue;
    private readonly Timer _flushTimer;
    private string _currentLogFile;

    public LogManager(string logDirectory = "logs")
    {
        _logDirectory = logDirectory;
        _logQueue = new Queue<LogEntry>();
        Directory.CreateDirectory(_logDirectory);
        _currentLogFile = GetLogFileName();

        _flushTimer = new Timer(FlushLogs, null, TimeSpan.FromSeconds(5), TimeSpan.FromSeconds(5));
    }

    public void WriteLog(LogLevel level, string module, string message)
    {
        var logEntry = new LogEntry
        {
            Timestamp = DateTime.UtcNow,
            Level = level,
            Module = module,
            Message = message,
            ThreadId = Thread.CurrentThread.ManagedThreadId
        };

        lock (_lockObject)
        {
            _logQueue.Enqueue(logEntry);
        }
    }

    private void FlushLogs(object state)
    {
        List<LogEntry> logsToWrite = new List<LogEntry>();

        lock (_lockObject)
        {
            while (_logQueue.Count > 0)
            {
                logsToWrite.Add(_logQueue.Dequeue());
            }
        }

        if (logsToWrite.Any())
        {
            var filePath = Path.Combine(_logDirectory, _currentLogFile);
            var logLines = logsToWrite.Select(FormatLogEntry);
            File.AppendAllLines(filePath, logLines);

            CheckLogRotation(filePath);
        }
    }

    private string FormatLogEntry(LogEntry entry)
    {
        return $"[{entry.Timestamp:yyyy-MM-dd HH:mm:ss.fff}] [{entry.Level}] [{entry.Module}] [{entry.ThreadId}] {entry.Message}";
    }

    private void CheckLogRotation(string filePath)
    {
        var fileInfo = new FileInfo(filePath);
        if (fileInfo.Length > 10 * 1024 * 1024) // 10MB
        {
            _currentLogFile = GetLogFileName();
        }
    }

    private string GetLogFileName()
    {
        return $"app_{DateTime.Now:yyyyMMdd_HHmmss}.log";
    }
}

public class LogEntry
{
    public DateTime Timestamp { get; set; }
    public LogLevel Level { get; set; }
    public string Module { get; set; }
    public string Message { get; set; }
    public int ThreadId { get; set; }
}

public enum LogLevel
{
    Debug, Info, Warning, Error, Fatal
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 90
```python
import os
import threading
import time
import queue
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from typing import Optional

class LogLevel(Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    FATAL = "FATAL"

@dataclass
class LogEntry:
    timestamp: datetime
    level: LogLevel
    module: str
    message: str
    thread_id: int

class LogManager:
    def __init__(self, log_directory: str = "logs"):
        self.log_directory = log_directory
        self.log_queue = queue.Queue()
        self.current_log_file = self._get_log_filename()
        self.running = True

        os.makedirs(log_directory, exist_ok=True)

        # Start background thread for writing logs
        self.writer_thread = threading.Thread(target=self._log_writer, daemon=True)
        self.writer_thread.start()

    def write_log(self, level: LogLevel, module: str, message: str):
        log_entry = LogEntry(
            timestamp=datetime.utcnow(),
            level=level,
            module=module,
            message=message,
            thread_id=threading.get_ident()
        )
        self.log_queue.put(log_entry)

    def _log_writer(self):
        while self.running:
            try:
                logs_batch = []

                # Collect logs for batch writing
                while len(logs_batch) < 100:  # Max batch size
                    try:
                        log_entry = self.log_queue.get(timeout=1)
                        logs_batch.append(log_entry)
                    except queue.Empty:
                        break

                if logs_batch:
                    self._write_logs_to_file(logs_batch)

            except Exception as e:
                print(f"Error in log writer: {e}")

    def _write_logs_to_file(self, logs: list[LogEntry]):
        file_path = os.path.join(self.log_directory, self.current_log_file)

        with open(file_path, 'a', encoding='utf-8') as f:
            for log_entry in logs:
                formatted_log = self._format_log_entry(log_entry)
                f.write(formatted_log + '\n')

        self._check_log_rotation(file_path)

    def _format_log_entry(self, entry: LogEntry) -> str:
        return (f"[{entry.timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}] "
                f"[{entry.level.value}] [{entry.module}] [{entry.thread_id}] {entry.message}")

    def _check_log_rotation(self, file_path: str):
        if os.path.getsize(file_path) > 10 * 1024 * 1024:  # 10MB
            self.current_log_file = self._get_log_filename()

    def _get_log_filename(self) -> str:
        return f"app_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    def shutdown(self):
        self.running = False
        self.writer_thread.join()
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 106
```typescript
import fs from 'fs';
import path from 'path';

enum LogLevel {
    DEBUG = 'DEBUG',
    INFO = 'INFO',
    WARNING = 'WARNING',
    ERROR = 'ERROR',
    FATAL = 'FATAL'
}

interface LogEntry {
    timestamp: Date;
    level: LogLevel;
    module: string;
    message: string;
    threadId: string;
}

class LogManager {
    private logDirectory: string;
    private logQueue: LogEntry[] = [];
    private currentLogFile: string;
    private flushInterval: NodeJS.Timeout;

    constructor(logDirectory: string = 'logs') {
        this.logDirectory = logDirectory;
        this.currentLogFile = this.getLogFileName();

        if (!fs.existsSync(logDirectory)) {
            fs.mkdirSync(logDirectory, { recursive: true });
        }

        // Flush logs every 5 seconds
        this.flushInterval = setInterval(() => {
            this.flushLogs();
        }, 5000);
    }

    writeLog(level: LogLevel, module: string, message: string): void {
        const logEntry: LogEntry = {
            timestamp: new Date(),
            level,
            module,
            message,
            threadId: process.pid.toString()
        };

        this.logQueue.push(logEntry);

        // Auto-flush on high priority logs
        if (level === LogLevel.ERROR || level === LogLevel.FATAL) {
            this.flushLogs();
        }
    }

    private flushLogs(): void {
        if (this.logQueue.length === 0) return;

        const logsToWrite = [...this.logQueue];
        this.logQueue = [];

        const filePath = path.join(this.logDirectory, this.currentLogFile);
        const logLines = logsToWrite.map(entry => this.formatLogEntry(entry));

        fs.appendFileSync(filePath, logLines.join('\n') + '\n');

        this.checkLogRotation(filePath);
    }

    private formatLogEntry(entry: LogEntry): string {
        const timestamp = entry.timestamp.toISOString().replace('T', ' ').slice(0, -1);
        return `[${timestamp}] [${entry.level}] [${entry.module}] [${entry.threadId}] ${entry.message}`;
    }

    private checkLogRotation(filePath: string): void {
        const stats = fs.statSync(filePath);
        if (stats.size > 10 * 1024 * 1024) { // 10MB
            this.currentLogFile = this.getLogFileName();
        }
    }

    private getLogFileName(): string {
        const now = new Date();
        const dateStr = now.toISOString().slice(0, 19).replace(/[:-]/g, '').replace('T', '_');
        return `app_${dateStr}.log`;
    }

    shutdown(): void {
        clearInterval(this.flushInterval);
        this.flushLogs(); // Final flush
    }
}

// Usage example
const logger = new LogManager();

// Export functions for different modules
export const logInfo = (module: string, message: string) =>
    logger.writeLog(LogLevel.INFO, module, message);

export const logError = (module: string, message: string) =>
    logger.writeLog(LogLevel.ERROR, module, message);

export const logWarning = (module: string, message: string) =>
    logger.writeLog(LogLevel.WARNING, module, message);
```

### ðŸ§ª Senaryo 4: Bir web uygulamasÄ±nda kullanÄ±cÄ±lar karmaÅŸÄ±k arama sorgularÄ± yapabiliyor. Arama iÅŸlemi veritabanÄ±nda birden fazla tablo Ã¼zerinde Ã§alÄ±ÅŸÄ±r. DetaylÄ± arama kriterleri Ã§ok sayÄ±da kayÄ±t kontrolÃ¼ gerektirir. Arama sonuÃ§larÄ± kullanÄ±cÄ±ya liste halinde sunulur.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 96
```csharp
public class SearchService
{
    private readonly ApplicationDbContext _context;

    public SearchService(ApplicationDbContext context)
    {
        _context = context;
    }

    public async Task<SearchResult> SearchAsync(SearchCriteria criteria)
    {
        var query = BuildQuery(criteria);

        var totalCount = await query.CountAsync();
        var results = await query
            .Skip(criteria.PageNumber * criteria.PageSize)
            .Take(criteria.PageSize)
            .Select(p => new SearchResultItem
            {
                Id = p.Id,
                Title = p.Title,
                Description = p.Description,
                Category = p.Category.Name,
                Price = p.Price,
                Rating = p.Reviews.Any() ? p.Reviews.Average(r => r.Rating) : 0,
                ReviewCount = p.Reviews.Count(),
                ImageUrl = p.Images.FirstOrDefault().Url,
                IsInStock = p.Stock > 0
            })
            .ToListAsync();

        return new SearchResult
        {
            Items = results,
            TotalCount = totalCount,
            PageNumber = criteria.PageNumber,
            PageSize = criteria.PageSize,
            TotalPages = (int)Math.Ceiling((double)totalCount / criteria.PageSize)
        };
    }

    private IQueryable<Product> BuildQuery(SearchCriteria criteria)
    {
        var query = _context.Products
            .Include(p => p.Category)
            .Include(p => p.Reviews)
            .Include(p => p.Images)
            .AsQueryable();

        if (!string.IsNullOrEmpty(criteria.SearchTerm))
        {
            query = query.Where(p =>
                EF.Functions.Contains(p.Title, criteria.SearchTerm) ||
                EF.Functions.Contains(p.Description, criteria.SearchTerm) ||
                p.Tags.Any(t => EF.Functions.Contains(t.Name, criteria.SearchTerm)));
        }

        if (criteria.CategoryIds?.Any() == true)
        {
            query = query.Where(p => criteria.CategoryIds.Contains(p.CategoryId));
        }

        if (criteria.MinPrice.HasValue)
        {
            query = query.Where(p => p.Price >= criteria.MinPrice.Value);
        }

        if (criteria.MaxPrice.HasValue)
        {
            query = query.Where(p => p.Price <= criteria.MaxPrice.Value);
        }

        if (criteria.MinRating.HasValue)
        {
            query = query.Where(p => p.Reviews.Any() &&
                p.Reviews.Average(r => r.Rating) >= criteria.MinRating.Value);
        }

        if (criteria.InStockOnly)
        {
            query = query.Where(p => p.Stock > 0);
        }

        // Apply sorting
        query = criteria.SortBy switch
        {
            "price_asc" => query.OrderBy(p => p.Price),
            "price_desc" => query.OrderByDescending(p => p.Price),
            "rating" => query.OrderByDescending(p => p.Reviews.Any() ? p.Reviews.Average(r => r.Rating) : 0),
            "newest" => query.OrderByDescending(p => p.CreatedAt),
            _ => query.OrderBy(p => p.Title)
        };

        return query;
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 136
```python
from sqlalchemy.orm import Session, joinedload
from sqlalchemy import and_, or_, func, desc, asc
from dataclasses import dataclass
from typing import List, Optional
from models import Product, Category, Review, Tag, ProductImage

@dataclass
class SearchCriteria:
    search_term: Optional[str] = None
    category_ids: Optional[List[int]] = None
    min_price: Optional[float] = None
    max_price: Optional[float] = None
    min_rating: Optional[float] = None
    in_stock_only: bool = False
    sort_by: str = "relevance"
    page_number: int = 0
    page_size: int = 20

@dataclass
class SearchResultItem:
    id: int
    title: str
    description: str
    category: str
    price: float
    rating: float
    review_count: int
    image_url: Optional[str]
    is_in_stock: bool

@dataclass
class SearchResult:
    items: List[SearchResultItem]
    total_count: int
    page_number: int
    page_size: int
    total_pages: int

class SearchService:
    def __init__(self, db: Session):
        self.db = db

    async def search(self, criteria: SearchCriteria) -> SearchResult:
        query = self._build_query(criteria)

        # Count total results
        total_count = query.count()

        # Apply pagination and get results
        results = query.offset(criteria.page_number * criteria.page_size)\
                      .limit(criteria.page_size)\
                      .all()

        items = [
            SearchResultItem(
                id=product.id,
                title=product.title,
                description=product.description,
                category=product.category.name,
                price=product.price,
                rating=self._calculate_rating(product.reviews),
                review_count=len(product.reviews),
                image_url=product.images[0].url if product.images else None,
                is_in_stock=product.stock > 0
            )
            for product in results
        ]

        total_pages = (total_count + criteria.page_size - 1) // criteria.page_size

        return SearchResult(
            items=items,
            total_count=total_count,
            page_number=criteria.page_number,
            page_size=criteria.page_size,
            total_pages=total_pages
        )

    def _build_query(self, criteria: SearchCriteria):
        query = self.db.query(Product)\
                      .options(
                          joinedload(Product.category),
                          joinedload(Product.reviews),
                          joinedload(Product.images)
                      )

        # Search term filtering
        if criteria.search_term:
            search_filter = or_(
                Product.title.ilike(f'%{criteria.search_term}%'),
                Product.description.ilike(f'%{criteria.search_term}%'),
                Product.tags.any(Tag.name.ilike(f'%{criteria.search_term}%'))
            )
            query = query.filter(search_filter)

        # Category filtering
        if criteria.category_ids:
            query = query.filter(Product.category_id.in_(criteria.category_ids))

        # Price filtering
        if criteria.min_price is not None:
            query = query.filter(Product.price >= criteria.min_price)

        if criteria.max_price is not None:
            query = query.filter(Product.price <= criteria.max_price)

        # Rating filtering
        if criteria.min_rating is not None:
            query = query.join(Review)\
                        .group_by(Product.id)\
                        .having(func.avg(Review.rating) >= criteria.min_rating)

        # Stock filtering
        if criteria.in_stock_only:
            query = query.filter(Product.stock > 0)

        # Sorting
        if criteria.sort_by == "price_asc":
            query = query.order_by(asc(Product.price))
        elif criteria.sort_by == "price_desc":
            query = query.order_by(desc(Product.price))
        elif criteria.sort_by == "rating":
            query = query.outerjoin(Review)\
                        .group_by(Product.id)\
                        .order_by(desc(func.avg(Review.rating)))
        elif criteria.sort_by == "newest":
            query = query.order_by(desc(Product.created_at))
        else:
            query = query.order_by(Product.title)

        return query

    def _calculate_rating(self, reviews) -> float:
        if not reviews:
            return 0.0
        return sum(review.rating for review in reviews) / len(reviews)
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 154
```typescript
import { Repository, SelectQueryBuilder } from 'typeorm';
import { Product } from './entities/Product';
import { Category } from './entities/Category';
import { Review } from './entities/Review';

interface SearchCriteria {
    searchTerm?: string;
    categoryIds?: number[];
    minPrice?: number;
    maxPrice?: number;
    minRating?: number;
    inStockOnly?: boolean;
    sortBy?: string;
    pageNumber: number;
    pageSize: number;
}

interface SearchResultItem {
    id: number;
    title: string;
    description: string;
    category: string;
    price: number;
    rating: number;
    reviewCount: number;
    imageUrl?: string;
    isInStock: boolean;
}

interface SearchResult {
    items: SearchResultItem[];
    totalCount: number;
    pageNumber: number;
    pageSize: number;
    totalPages: number;
}

export class SearchService {
    constructor(private productRepository: Repository<Product>) {}

    async search(criteria: SearchCriteria): Promise<SearchResult> {
        const queryBuilder = this.buildQuery(criteria);

        // Get total count
        const totalCount = await queryBuilder.getCount();

        // Apply pagination and get results
        const products = await queryBuilder
            .skip(criteria.pageNumber * criteria.pageSize)
            .take(criteria.pageSize)
            .getMany();

        const items: SearchResultItem[] = products.map(product => ({
            id: product.id,
            title: product.title,
            description: product.description,
            category: product.category.name,
            price: product.price,
            rating: this.calculateRating(product.reviews),
            reviewCount: product.reviews.length,
            imageUrl: product.images?.[0]?.url,
            isInStock: product.stock > 0
        }));

        const totalPages = Math.ceil(totalCount / criteria.pageSize);

        return {
            items,
            totalCount,
            pageNumber: criteria.pageNumber,
            pageSize: criteria.pageSize,
            totalPages
        };
    }

    private buildQuery(criteria: SearchCriteria): SelectQueryBuilder<Product> {
        let query = this.productRepository
            .createQueryBuilder('product')
            .leftJoinAndSelect('product.category', 'category')
            .leftJoinAndSelect('product.reviews', 'reviews')
            .leftJoinAndSelect('product.images', 'images')
            .leftJoinAndSelect('product.tags', 'tags');

        // Search term filtering
        if (criteria.searchTerm) {
            query = query.andWhere(
                '(product.title ILIKE :searchTerm OR ' +
                'product.description ILIKE :searchTerm OR ' +
                'tags.name ILIKE :searchTerm)',
                { searchTerm: `%${criteria.searchTerm}%` }
            );
        }

        // Category filtering
        if (criteria.categoryIds?.length) {
            query = query.andWhere('product.categoryId IN (:...categoryIds)', {
                categoryIds: criteria.categoryIds
            });
        }

        // Price filtering
        if (criteria.minPrice !== undefined) {
            query = query.andWhere('product.price >= :minPrice', {
                minPrice: criteria.minPrice
            });
        }

        if (criteria.maxPrice !== undefined) {
            query = query.andWhere('product.price <= :maxPrice', {
                maxPrice: criteria.maxPrice
            });
        }

        // Rating filtering
        if (criteria.minRating !== undefined) {
            query = query
                .groupBy('product.id, category.id')
                .having('AVG(reviews.rating) >= :minRating', {
                    minRating: criteria.minRating
                });
        }

        // Stock filtering
        if (criteria.inStockOnly) {
            query = query.andWhere('product.stock > 0');
        }

        // Sorting
        switch (criteria.sortBy) {
            case 'price_asc':
                query = query.orderBy('product.price', 'ASC');
                break;
            case 'price_desc':
                query = query.orderBy('product.price', 'DESC');
                break;
            case 'rating':
                query = query.orderBy('AVG(reviews.rating)', 'DESC');
                break;
            case 'newest':
                query = query.orderBy('product.createdAt', 'DESC');
                break;
            default:
                query = query.orderBy('product.title', 'ASC');
        }

        return query;
    }

    private calculateRating(reviews: Review[]): number {
        if (!reviews || reviews.length === 0) return 0;
        const sum = reviews.reduce((acc, review) => acc + review.rating, 0);
        return sum / reviews.length;
    }
}
```

### ðŸ§ª Senaryo 5: Bir video iÅŸleme uygulamasÄ± kullanÄ±cÄ±larÄ±n yÃ¼klediÄŸi videolarÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor. FarklÄ± formatlardaki videolar sisteme yÃ¼klenebilir. DÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi sÄ±rasÄ±nda Ã§eÅŸitli kodekler kullanÄ±lÄ±r. Ä°ÅŸlenmiÅŸ videolar kullanÄ±cÄ±lara sunulur.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 127
```csharp
public class VideoProcessingService
{
    private readonly ILogger<VideoProcessingService> _logger;
    private readonly ApplicationDbContext _context;
    private readonly string _inputPath;
    private readonly string _outputPath;

    public VideoProcessingService(ILogger<VideoProcessingService> logger,
                                ApplicationDbContext context,
                                IConfiguration config)
    {
        _logger = logger;
        _context = context;
        _inputPath = config["VideoProcessing:InputPath"];
        _outputPath = config["VideoProcessing:OutputPath"];
    }

    public async Task<VideoProcessingResult> ProcessVideoAsync(int videoId, VideoConversionOptions options)
    {
        var video = await _context.Videos.FindAsync(videoId);
        if (video == null)
            throw new ArgumentException("Video not found");

        _logger.LogInformation($"Starting video processing for {video.FileName}");

        var inputFile = Path.Combine(_inputPath, video.FileName);
        var outputFileName = GenerateOutputFileName(video.FileName, options.Format);
        var outputFile = Path.Combine(_outputPath, outputFileName);

        try
        {
            video.Status = VideoStatus.Processing;
            video.StartedAt = DateTime.UtcNow;
            await _context.SaveChangesAsync();

            var ffmpegArgs = BuildFFmpegArguments(inputFile, outputFile, options);

            using var process = new Process
            {
                StartInfo = new ProcessStartInfo
                {
                    FileName = "ffmpeg",
                    Arguments = ffmpegArgs,
                    UseShellExecute = false,
                    RedirectStandardOutput = true,
                    RedirectStandardError = true,
                    CreateNoWindow = true
                }
            };

            var progressHandler = new VideoProcessingProgressHandler(video.Id, _context);
            process.ErrorDataReceived += progressHandler.HandleProgressUpdate;

            process.Start();
            process.BeginErrorReadLine();
            await process.WaitForExitAsync();

            if (process.ExitCode == 0)
            {
                video.Status = VideoStatus.Completed;
                video.OutputFileName = outputFileName;
                video.CompletedAt = DateTime.UtcNow;
                video.OutputFileSize = new FileInfo(outputFile).Length;

                await _context.SaveChangesAsync();

                _logger.LogInformation($"Video processing completed: {outputFileName}");

                return new VideoProcessingResult
                {
                    Success = true,
                    OutputFile = outputFileName,
                    ProcessingTime = video.CompletedAt.Value - video.StartedAt.Value
                };
            }
            else
            {
                video.Status = VideoStatus.Failed;
                video.ErrorMessage = "FFmpeg processing failed";
                await _context.SaveChangesAsync();

                return new VideoProcessingResult { Success = false, Error = "Processing failed" };
            }
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, $"Error processing video {videoId}");

            video.Status = VideoStatus.Failed;
            video.ErrorMessage = ex.Message;
            await _context.SaveChangesAsync();

            return new VideoProcessingResult { Success = false, Error = ex.Message };
        }
    }

    private string BuildFFmpegArguments(string inputFile, string outputFile, VideoConversionOptions options)
    {
        var args = new List<string>
        {
            "-i", $"\"{inputFile}\"",
            "-c:v", options.VideoCodec,
            "-c:a", options.AudioCodec
        };

        if (options.Resolution != null)
        {
            args.AddRange(["-s", options.Resolution]);
        }

        if (options.Bitrate != null)
        {
            args.AddRange(["-b:v", options.Bitrate]);
        }

        args.AddRange(["-y", $"\"{outputFile}\""]);

        return string.Join(" ", args);
    }

    private string GenerateOutputFileName(string originalFileName, string format)
    {
        var nameWithoutExt = Path.GetFileNameWithoutExtension(originalFileName);
        var timestamp = DateTime.Now.ToString("yyyyMMdd_HHmmss");
        return $"{nameWithoutExt}_{timestamp}.{format}";
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 164
```python
import asyncio
import subprocess
import logging
import os
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from typing import Optional
from sqlalchemy.orm import Session

class VideoStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class VideoConversionOptions:
    format: str
    video_codec: str = "libx264"
    audio_codec: str = "aac"
    resolution: Optional[str] = None
    bitrate: Optional[str] = None
    quality: str = "medium"

@dataclass
class VideoProcessingResult:
    success: bool
    output_file: Optional[str] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

class VideoProcessingService:
    def __init__(self, db: Session, input_path: str, output_path: str):
        self.db = db
        self.input_path = input_path
        self.output_path = output_path
        self.logger = logging.getLogger(__name__)

        os.makedirs(input_path, exist_ok=True)
        os.makedirs(output_path, exist_ok=True)

    async def process_video(self, video_id: int, options: VideoConversionOptions) -> VideoProcessingResult:
        video = self.db.query(Video).filter(Video.id == video_id).first()
        if not video:
            raise ValueError("Video not found")

        self.logger.info(f"Starting video processing for {video.file_name}")

        input_file = os.path.join(self.input_path, video.file_name)
        output_file_name = self._generate_output_filename(video.file_name, options.format)
        output_file = os.path.join(self.output_path, output_file_name)

        try:
            # Update video status
            video.status = VideoStatus.PROCESSING
            video.started_at = datetime.utcnow()
            self.db.commit()

            # Build FFmpeg command
            ffmpeg_cmd = self._build_ffmpeg_command(input_file, output_file, options)

            start_time = datetime.utcnow()

            # Execute FFmpeg process
            process = await asyncio.create_subprocess_exec(
                *ffmpeg_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            end_time = datetime.utcnow()
            processing_time = (end_time - start_time).total_seconds()

            if process.returncode == 0:
                # Success
                video.status = VideoStatus.COMPLETED
                video.output_file_name = output_file_name
                video.completed_at = datetime.utcnow()
                video.output_file_size = os.path.getsize(output_file)
                self.db.commit()

                self.logger.info(f"Video processing completed: {output_file_name}")

                return VideoProcessingResult(
                    success=True,
                    output_file=output_file_name,
                    processing_time=processing_time
                )
            else:
                # Failed
                error_msg = stderr.decode('utf-8') if stderr else "Unknown error"
                video.status = VideoStatus.FAILED
                video.error_message = error_msg
                self.db.commit()

                return VideoProcessingResult(success=False, error=error_msg)

        except Exception as e:
            self.logger.error(f"Error processing video {video_id}: {str(e)}")

            video.status = VideoStatus.FAILED
            video.error_message = str(e)
            self.db.commit()

            return VideoProcessingResult(success=False, error=str(e))

    def _build_ffmpeg_command(self, input_file: str, output_file: str, options: VideoConversionOptions) -> list:
        cmd = [
            "ffmpeg",
            "-i", input_file,
            "-c:v", options.video_codec,
            "-c:a", options.audio_codec
        ]

        if options.resolution:
            cmd.extend(["-s", options.resolution])

        if options.bitrate:
            cmd.extend(["-b:v", options.bitrate])

        # Quality presets
        if options.quality == "high":
            cmd.extend(["-crf", "18"])
        elif options.quality == "medium":
            cmd.extend(["-crf", "23"])
        elif options.quality == "low":
            cmd.extend(["-crf", "28"])

        cmd.extend(["-y", output_file])  # -y to overwrite output file

        return cmd

    def _generate_output_filename(self, original_filename: str, format: str) -> str:
        name_without_ext = os.path.splitext(original_filename)[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{name_without_ext}_{timestamp}.{format}"

    async def get_video_info(self, video_path: str) -> dict:
        """Get video information using ffprobe"""
        cmd = [
            "ffprobe",
            "-v", "quiet",
            "-print_format", "json",
            "-show_format",
            "-show_streams",
            video_path
        ]

        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        stdout, _ = await process.communicate()

        if process.returncode == 0:
            import json
            return json.loads(stdout.decode('utf-8'))

        return {}
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 214
```typescript
import { spawn } from 'child_process';
import { promises as fs } from 'fs';
import path from 'path';
import { VideoRepository } from './repositories/VideoRepository';

enum VideoStatus {
    PENDING = 'pending',
    PROCESSING = 'processing',
    COMPLETED = 'completed',
    FAILED = 'failed'
}

interface VideoConversionOptions {
    format: string;
    videoCodec?: string;
    audioCodec?: string;
    resolution?: string;
    bitrate?: string;
    quality?: 'low' | 'medium' | 'high';
}

interface VideoProcessingResult {
    success: boolean;
    outputFile?: string;
    processingTime?: number;
    error?: string;
}

export class VideoProcessingService {
    private videoRepository: VideoRepository;
    private inputPath: string;
    private outputPath: string;

    constructor(videoRepository: VideoRepository, inputPath: string, outputPath: string) {
        this.videoRepository = videoRepository;
        this.inputPath = inputPath;
        this.outputPath = outputPath;
    }

    async processVideo(videoId: number, options: VideoConversionOptions): Promise<VideoProcessingResult> {
        const video = await this.videoRepository.findById(videoId);
        if (!video) {
            throw new Error('Video not found');
        }

        console.log(`Starting video processing for ${video.fileName}`);

        const inputFile = path.join(this.inputPath, video.fileName);
        const outputFileName = this.generateOutputFileName(video.fileName, options.format);
        const outputFile = path.join(this.outputPath, outputFileName);

        try {
            // Update video status
            await this.videoRepository.updateStatus(videoId, VideoStatus.PROCESSING);

            const startTime = Date.now();

            // Build FFmpeg arguments
            const ffmpegArgs = this.buildFFmpegArguments(inputFile, outputFile, options);

            // Execute FFmpeg process
            const result = await this.executeFFmpeg(ffmpegArgs);

            const endTime = Date.now();
            const processingTime = (endTime - startTime) / 1000; // Convert to seconds

            if (result.success) {
                const fileStats = await fs.stat(outputFile);

                await this.videoRepository.updateCompleted(videoId, {
                    status: VideoStatus.COMPLETED,
                    outputFileName,
                    outputFileSize: fileStats.size,
                    completedAt: new Date()
                });

                console.log(`Video processing completed: ${outputFileName}`);

                return {
                    success: true,
                    outputFile: outputFileName,
                    processingTime
                };
            } else {
                await this.videoRepository.updateFailed(videoId, result.error || 'Processing failed');

                return {
                    success: false,
                    error: result.error
                };
            }

        } catch (error) {
            console.error(`Error processing video ${videoId}:`, error);

            await this.videoRepository.updateFailed(videoId, error.message);

            return {
                success: false,
                error: error.message
            };
        }
    }

    private buildFFmpegArguments(inputFile: string, outputFile: string, options: VideoConversionOptions): string[] {
        const args = [
            '-i', inputFile,
            '-c:v', options.videoCodec || 'libx264',
            '-c:a', options.audioCodec || 'aac'
        ];

        if (options.resolution) {
            args.push('-s', options.resolution);
        }

        if (options.bitrate) {
            args.push('-b:v', options.bitrate);
        }

        // Quality settings
        switch (options.quality) {
            case 'high':
                args.push('-crf', '18');
                break;
            case 'medium':
                args.push('-crf', '23');
                break;
            case 'low':
                args.push('-crf', '28');
                break;
        }

        args.push('-y', outputFile); // Overwrite output file if exists

        return args;
    }

    private executeFFmpeg(args: string[]): Promise<{ success: boolean; error?: string }> {
        return new Promise((resolve) => {
            const ffmpeg = spawn('ffmpeg', args);
            let errorOutput = '';

            ffmpeg.stderr.on('data', (data) => {
                errorOutput += data.toString();
                // Parse progress information if needed
                this.parseProgress(data.toString());
            });

            ffmpeg.on('close', (code) => {
                if (code === 0) {
                    resolve({ success: true });
                } else {
                    resolve({
                        success: false,
                        error: errorOutput || `FFmpeg exited with code ${code}`
                    });
                }
            });

            ffmpeg.on('error', (error) => {
                resolve({
                    success: false,
                    error: `Failed to start FFmpeg: ${error.message}`
                });
            });
        });
    }

    private parseProgress(output: string): void {
        // Parse FFmpeg progress output
        const timeMatch = output.match(/time=(\d{2}):(\d{2}):(\d{2}\.\d{2})/);
        if (timeMatch) {
            const [, hours, minutes, seconds] = timeMatch;
            const currentTime = parseInt(hours) * 3600 + parseInt(minutes) * 60 + parseFloat(seconds);
            // Update progress in database or emit progress event
            console.log(`Processing progress: ${currentTime}s`);
        }
    }

    private generateOutputFileName(originalFileName: string, format: string): string {
        const nameWithoutExt = path.parse(originalFileName).name;
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
        return `${nameWithoutExt}_${timestamp}.${format}`;
    }

    async getVideoInfo(videoPath: string): Promise<any> {
        return new Promise((resolve, reject) => {
            const ffprobe = spawn('ffprobe', [
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_format',
                '-show_streams',
                videoPath
            ]);

            let output = '';
            ffprobe.stdout.on('data', (data) => {
                output += data.toString();
            });

            ffprobe.on('close', (code) => {
                if (code === 0) {
                    try {
                        resolve(JSON.parse(output));
                    } catch (error) {
                        reject(error);
                    }
                } else {
                    reject(new Error(`ffprobe exited with code ${code}`));
                }
            });
        });
    }
}
```

### ðŸ§ª Senaryo 6: Bir sosyal medya platformu kullanÄ±cÄ±larÄ±n fotoÄŸraflarÄ±nÄ± yÃ¼klemelerine olanak tanÄ±yor. YÃ¼klenen fotoÄŸraflar farklÄ± boyutlarda olabilir. Sistem bu fotoÄŸraflarÄ± thumbnail oluÅŸturmak iÃ§in iÅŸler. Ä°ÅŸlenen fotoÄŸraflar kullanÄ±cÄ± profillerinde gÃ¶sterilir.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 172
```csharp
public class ImageProcessingService
{
    private readonly ILogger<ImageProcessingService> _logger;
    private readonly ApplicationDbContext _context;
    private readonly string _uploadsPath;
    private readonly string _thumbnailsPath;

    public ImageProcessingService(ILogger<ImageProcessingService> logger,
                                ApplicationDbContext context,
                                IConfiguration config)
    {
        _logger = logger;
        _context = context;
        _uploadsPath = config["Images:UploadsPath"];
        _thumbnailsPath = config["Images:ThumbnailsPath"];

        Directory.CreateDirectory(_uploadsPath);
        Directory.CreateDirectory(_thumbnailsPath);
    }

    public async Task<ImageProcessingResult> ProcessImageAsync(IFormFile file, int userId)
    {
        if (file == null || file.Length == 0)
            throw new ArgumentException("No file provided");

        var allowedTypes = new[] { "image/jpeg", "image/png", "image/webp" };
        if (!allowedTypes.Contains(file.ContentType))
            throw new ArgumentException("Invalid file type");

        try
        {
            var imageId = Guid.NewGuid();
            var fileName = $"{imageId}{Path.GetExtension(file.FileName)}";
            var filePath = Path.Combine(_uploadsPath, fileName);

            // Save original image
            using (var stream = new FileStream(filePath, FileMode.Create))
            {
                await file.CopyToAsync(stream);
            }

            // Process image and create thumbnails
            var thumbnails = await CreateThumbnailsAsync(filePath, imageId.ToString());

            // Save to database
            var image = new UserImage
            {
                Id = imageId,
                UserId = userId,
                OriginalFileName = file.FileName,
                FileName = fileName,
                FilePath = filePath,
                FileSize = file.Length,
                ContentType = file.ContentType,
                UploadedAt = DateTime.UtcNow,
                Thumbnails = thumbnails.Select(t => new ImageThumbnail
                {
                    Size = t.Size,
                    FileName = t.FileName,
                    FilePath = t.FilePath,
                    Width = t.Width,
                    Height = t.Height
                }).ToList()
            };

            _context.UserImages.Add(image);
            await _context.SaveChangesAsync();

            _logger.LogInformation($"Image processed successfully: {fileName}");

            return new ImageProcessingResult
            {
                Success = true,
                ImageId = imageId,
                OriginalFile = fileName,
                Thumbnails = thumbnails
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Error processing image");
            throw;
        }
    }

    private async Task<List<ThumbnailInfo>> CreateThumbnailsAsync(string originalPath, string imageId)
    {
        var thumbnailSizes = new[]
        {
            new { Name = "small", Width = 150, Height = 150 },
            new { Name = "medium", Width = 300, Height = 300 },
            new { Name = "large", Width = 800, Height = 600 }
        };

        var thumbnails = new List<ThumbnailInfo>();

        using var originalImage = await Image.LoadAsync(originalPath);

        foreach (var size in thumbnailSizes)
        {
            var thumbnailFileName = $"{imageId}_{size.Name}.jpg";
            var thumbnailPath = Path.Combine(_thumbnailsPath, thumbnailFileName);

            using var thumbnail = originalImage.Clone(ctx =>
            {
                var resizeOptions = new ResizeOptions
                {
                    Size = new Size(size.Width, size.Height),
                    Mode = ResizeMode.Crop,
                    Position = AnchorPositionMode.Center
                };
                ctx.Resize(resizeOptions);
            });

            await thumbnail.SaveAsJpegAsync(thumbnailPath, new JpegEncoder { Quality = 85 });

            thumbnails.Add(new ThumbnailInfo
            {
                Size = size.Name,
                FileName = thumbnailFileName,
                FilePath = thumbnailPath,
                Width = size.Width,
                Height = size.Height
            });
        }

        return thumbnails;
    }

    public async Task<string> GetImageUrlAsync(Guid imageId, string size = "medium")
    {
        var image = await _context.UserImages
            .Include(i => i.Thumbnails)
            .FirstOrDefaultAsync(i => i.Id == imageId);

        if (image == null)
            return null;

        var thumbnail = image.Thumbnails.FirstOrDefault(t => t.Size == size);
        return thumbnail != null ? $"/images/thumbnails/{thumbnail.FileName}" : $"/images/{image.FileName}";
    }

    public async Task DeleteImageAsync(Guid imageId)
    {
        var image = await _context.UserImages
            .Include(i => i.Thumbnails)
            .FirstOrDefaultAsync(i => i.Id == imageId);

        if (image == null) return;

        // Delete physical files
        try
        {
            if (File.Exists(image.FilePath))
                File.Delete(image.FilePath);

            foreach (var thumbnail in image.Thumbnails)
            {
                if (File.Exists(thumbnail.FilePath))
                    File.Delete(thumbnail.FilePath);
            }
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, $"Error deleting image files for {imageId}");
        }

        // Delete from database
        _context.UserImages.Remove(image);
        await _context.SaveChangesAsync();
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 209
```python
import os
import uuid
from PIL import Image, ImageOps
from dataclasses import dataclass
from typing import List, Optional
from sqlalchemy.orm import Session
from fastapi import UploadFile, HTTPException
import aiofiles

@dataclass
class ThumbnailInfo:
    size: str
    filename: str
    file_path: str
    width: int
    height: int

@dataclass
class ImageProcessingResult:
    success: bool
    image_id: Optional[str] = None
    original_file: Optional[str] = None
    thumbnails: Optional[List[ThumbnailInfo]] = None
    error: Optional[str] = None

class ImageProcessingService:
    def __init__(self, db: Session, uploads_path: str, thumbnails_path: str):
        self.db = db
        self.uploads_path = uploads_path
        self.thumbnails_path = thumbnails_path

        os.makedirs(uploads_path, exist_ok=True)
        os.makedirs(thumbnails_path, exist_ok=True)

        self.allowed_types = ['image/jpeg', 'image/png', 'image/webp']
        self.thumbnail_sizes = [
            {'name': 'small', 'width': 150, 'height': 150},
            {'name': 'medium', 'width': 300, 'height': 300},
            {'name': 'large', 'width': 800, 'height': 600}
        ]

    async def process_image(self, file: UploadFile, user_id: int) -> ImageProcessingResult:
        if not file.filename:
            raise HTTPException(status_code=400, detail="No file provided")

        if file.content_type not in self.allowed_types:
            raise HTTPException(status_code=400, detail="Invalid file type")

        try:
            image_id = str(uuid.uuid4())
            file_extension = os.path.splitext(file.filename)[1]
            filename = f"{image_id}{file_extension}"
            file_path = os.path.join(self.uploads_path, filename)

            # Save original image
            async with aiofiles.open(file_path, 'wb') as f:
                content = await file.read()
                await f.write(content)

            # Create thumbnails
            thumbnails = await self.create_thumbnails(file_path, image_id)

            # Save to database
            user_image = UserImage(
                id=image_id,
                user_id=user_id,
                original_filename=file.filename,
                filename=filename,
                file_path=file_path,
                file_size=len(content),
                content_type=file.content_type,
                uploaded_at=datetime.utcnow()
            )

            self.db.add(user_image)

            for thumb in thumbnails:
                thumbnail_record = ImageThumbnail(
                    image_id=image_id,
                    size=thumb.size,
                    filename=thumb.filename,
                    file_path=thumb.file_path,
                    width=thumb.width,
                    height=thumb.height
                )
                self.db.add(thumbnail_record)

            self.db.commit()

            print(f"Image processed successfully: {filename}")

            return ImageProcessingResult(
                success=True,
                image_id=image_id,
                original_file=filename,
                thumbnails=thumbnails
            )

        except Exception as e:
            self.db.rollback()
            print(f"Error processing image: {str(e)}")
            return ImageProcessingResult(success=False, error=str(e))

    async def create_thumbnails(self, original_path: str, image_id: str) -> List[ThumbnailInfo]:
        thumbnails = []

        with Image.open(original_path) as original_image:
            # Convert to RGB if necessary
            if original_image.mode in ('RGBA', 'P'):
                original_image = original_image.convert('RGB')

            # Apply auto-orientation
            original_image = ImageOps.exif_transpose(original_image)

            for size_config in self.thumbnail_sizes:
                thumbnail_filename = f"{image_id}_{size_config['name']}.jpg"
                thumbnail_path = os.path.join(self.thumbnails_path, thumbnail_filename)

                # Calculate dimensions maintaining aspect ratio
                original_width, original_height = original_image.size
                target_width, target_height = size_config['width'], size_config['height']

                # Create thumbnail with crop
                thumbnail = ImageOps.fit(
                    original_image,
                    (target_width, target_height),
                    Image.Resampling.LANCZOS
                )

                # Save thumbnail
                thumbnail.save(thumbnail_path, 'JPEG', quality=85, optimize=True)

                thumbnails.append(ThumbnailInfo(
                    size=size_config['name'],
                    filename=thumbnail_filename,
                    file_path=thumbnail_path,
                    width=target_width,
                    height=target_height
                ))

        return thumbnails

    async def get_image_url(self, image_id: str, size: str = "medium") -> Optional[str]:
        image = self.db.query(UserImage).filter(UserImage.id == image_id).first()
        if not image:
            return None

        thumbnail = self.db.query(ImageThumbnail).filter(
            ImageThumbnail.image_id == image_id,
            ImageThumbnail.size == size
        ).first()

        if thumbnail:
            return f"/images/thumbnails/{thumbnail.filename}"
        else:
            return f"/images/{image.filename}"

    async def delete_image(self, image_id: str) -> bool:
        try:
            image = self.db.query(UserImage).filter(UserImage.id == image_id).first()
            if not image:
                return False

            # Delete physical files
            if os.path.exists(image.file_path):
                os.remove(image.file_path)

            thumbnails = self.db.query(ImageThumbnail).filter(
                ImageThumbnail.image_id == image_id
            ).all()

            for thumbnail in thumbnails:
                if os.path.exists(thumbnail.file_path):
                    os.remove(thumbnail.file_path)

            # Delete from database
            self.db.query(ImageThumbnail).filter(ImageThumbnail.image_id == image_id).delete()
            self.db.delete(image)
            self.db.commit()

            return True

        except Exception as e:
            self.db.rollback()
            print(f"Error deleting image {image_id}: {str(e)}")
            return False

    def get_user_images(self, user_id: int, limit: int = 20, offset: int = 0) -> List[dict]:
        images = self.db.query(UserImage).filter(
            UserImage.user_id == user_id
        ).order_by(UserImage.uploaded_at.desc()).offset(offset).limit(limit).all()

        result = []
        for image in images:
            thumbnails = self.db.query(ImageThumbnail).filter(
                ImageThumbnail.image_id == image.id
            ).all()

            result.append({
                'id': image.id,
                'filename': image.filename,
                'uploaded_at': image.uploaded_at,
                'thumbnails': {
                    thumb.size: f"/images/thumbnails/{thumb.filename}"
                    for thumb in thumbnails
                }
            })

        return result
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 229
```typescript
import multer from 'multer';
import sharp from 'sharp';
import path from 'path';
import fs from 'fs/promises';
import { v4 as uuidv4 } from 'uuid';
import { UserImageRepository } from './repositories/UserImageRepository';

interface ThumbnailInfo {
    size: string;
    filename: string;
    filePath: string;
    width: number;
    height: number;
}

interface ImageProcessingResult {
    success: boolean;
    imageId?: string;
    originalFile?: string;
    thumbnails?: ThumbnailInfo[];
    error?: string;
}

export class ImageProcessingService {
    private userImageRepository: UserImageRepository;
    private uploadsPath: string;
    private thumbnailsPath: string;
    private allowedTypes = ['image/jpeg', 'image/png', 'image/webp'];
    private thumbnailSizes = [
        { name: 'small', width: 150, height: 150 },
        { name: 'medium', width: 300, height: 300 },
        { name: 'large', width: 800, height: 600 }
    ];

    constructor(userImageRepository: UserImageRepository, uploadsPath: string, thumbnailsPath: string) {
        this.userImageRepository = userImageRepository;
        this.uploadsPath = uploadsPath;
        this.thumbnailsPath = thumbnailsPath;

        this.ensureDirectoriesExist();
    }

    private async ensureDirectoriesExist(): Promise<void> {
        await fs.mkdir(this.uploadsPath, { recursive: true });
        await fs.mkdir(this.thumbnailsPath, { recursive: true });
    }

    async processImage(file: Express.Multer.File, userId: number): Promise<ImageProcessingResult> {
        if (!file) {
            throw new Error('No file provided');
        }

        if (!this.allowedTypes.includes(file.mimetype)) {
            throw new Error('Invalid file type');
        }

        try {
            const imageId = uuidv4();
            const fileExtension = path.extname(file.originalname);
            const filename = `${imageId}${fileExtension}`;
            const filePath = path.join(this.uploadsPath, filename);

            // Save original image
            await fs.writeFile(filePath, file.buffer);

            // Create thumbnails
            const thumbnails = await this.createThumbnails(filePath, imageId);

            // Save to database
            const userImage = {
                id: imageId,
                userId,
                originalFilename: file.originalname,
                filename,
                filePath,
                fileSize: file.size,
                contentType: file.mimetype,
                uploadedAt: new Date()
            };

            await this.userImageRepository.create(userImage);

            // Save thumbnail records
            for (const thumbnail of thumbnails) {
                await this.userImageRepository.createThumbnail({
                    imageId,
                    size: thumbnail.size,
                    filename: thumbnail.filename,
                    filePath: thumbnail.filePath,
                    width: thumbnail.width,
                    height: thumbnail.height
                });
            }

            console.log(`Image processed successfully: ${filename}`);

            return {
                success: true,
                imageId,
                originalFile: filename,
                thumbnails
            };

        } catch (error) {
            console.error('Error processing image:', error);
            return {
                success: false,
                error: error.message
            };
        }
    }

    private async createThumbnails(originalPath: string, imageId: string): Promise<ThumbnailInfo[]> {
        const thumbnails: ThumbnailInfo[] = [];

        for (const sizeConfig of this.thumbnailSizes) {
            const thumbnailFilename = `${imageId}_${sizeConfig.name}.jpg`;
            const thumbnailPath = path.join(this.thumbnailsPath, thumbnailFilename);

            await sharp(originalPath)
                .resize(sizeConfig.width, sizeConfig.height, {
                    fit: 'cover',
                    position: 'center'
                })
                .jpeg({ quality: 85 })
                .toFile(thumbnailPath);

            thumbnails.push({
                size: sizeConfig.name,
                filename: thumbnailFilename,
                filePath: thumbnailPath,
                width: sizeConfig.width,
                height: sizeConfig.height
            });
        }

        return thumbnails;
    }

    async getImageUrl(imageId: string, size: string = 'medium'): Promise<string | null> {
        const image = await this.userImageRepository.findById(imageId);
        if (!image) {
            return null;
        }

        const thumbnail = await this.userImageRepository.getThumbnail(imageId, size);
        if (thumbnail) {
            return `/images/thumbnails/${thumbnail.filename}`;
        } else {
            return `/images/${image.filename}`;
        }
    }

    async deleteImage(imageId: string): Promise<boolean> {
        try {
            const image = await this.userImageRepository.findById(imageId);
            if (!image) {
                return false;
            }

            // Delete physical files
            try {
                await fs.unlink(image.filePath);
            } catch (error) {
                console.warn(`Failed to delete original file: ${error.message}`);
            }

            const thumbnails = await this.userImageRepository.getThumbnails(imageId);
            for (const thumbnail of thumbnails) {
                try {
                    await fs.unlink(thumbnail.filePath);
                } catch (error) {
                    console.warn(`Failed to delete thumbnail: ${error.message}`);
                }
            }

            // Delete from database
            await this.userImageRepository.delete(imageId);

            return true;

        } catch (error) {
            console.error(`Error deleting image ${imageId}:`, error);
            return false;
        }
    }

    async getUserImages(userId: number, limit: number = 20, offset: number = 0): Promise<any[]> {
        const images = await this.userImageRepository.findByUserId(userId, limit, offset);

        const result = [];
        for (const image of images) {
            const thumbnails = await this.userImageRepository.getThumbnails(image.id);

            const thumbnailUrls: Record<string, string> = {};
            for (const thumbnail of thumbnails) {
                thumbnailUrls[thumbnail.size] = `/images/thumbnails/${thumbnail.filename}`;
            }

            result.push({
                id: image.id,
                filename: image.filename,
                uploadedAt: image.uploadedAt,
                thumbnails: thumbnailUrls
            });
        }

        return result;
    }

    // Multer configuration for handling file uploads
    getMulterConfig(): multer.Multer {
        const storage = multer.memoryStorage();

        return multer({
            storage,
            limits: {
                fileSize: 10 * 1024 * 1024 // 10MB limit
            },
            fileFilter: (req, file, cb) => {
                if (this.allowedTypes.includes(file.mimetype)) {
                    cb(null, true);
                } else {
                    cb(new Error('Invalid file type'));
                }
            }
        });
    }
}
```

### ðŸ§ª Senaryo 7: Bir oyun sunucusu aynÄ± anda Ã§ok sayÄ±da oyuncuya hizmet verebiliyor. Her oyuncu sunucuyla aktif baÄŸlantÄ± kurar. Oyun sÄ±rasÄ±nda oyuncular arasÄ±nda sÃ¼rekli veri alÄ±ÅŸveriÅŸi yapÄ±lÄ±r. Sunucu tÃ¼m oyuncu verilerini bellekte tutar.

**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 64
```csharp
using System.Collections.Concurrent;
using System.Net.WebSockets;

public class GameServer
{
    private ConcurrentDictionary<string, PlayerData> players = new();
    private ConcurrentDictionary<string, WebSocket> connections = new();

    public async Task HandleConnection(WebSocket socket, string playerId)
    {
        // Vulnerable: No connection limit per player
        connections[playerId] = socket;
        players[playerId] = new PlayerData
        {
            Id = playerId,
            Position = new Vector3(0, 0, 0),
            Health = 100,
            Inventory = new List<Item>()
        };

        var buffer = new byte[4096];
        while (socket.State == WebSocketState.Open)
        {
            var result = await socket.ReceiveAsync(buffer, CancellationToken.None);
            // Process game data without validation
            await ProcessGameData(playerId, buffer, result.Count);
        }
    }

    private async Task ProcessGameData(string playerId, byte[] data, int length)
    {
        // Vulnerable: No rate limiting or data size validation
        var gameAction = Encoding.UTF8.GetString(data, 0, length);

        // Store all player actions in memory
        if (players.ContainsKey(playerId))
        {
            players[playerId].LastActions.Add(gameAction);
        }

        // Broadcast to all players without filtering
        await BroadcastToAllPlayers(data, length);
    }

    private async Task BroadcastToAllPlayers(byte[] data, int length)
    {
        foreach (var connection in connections.Values)
        {
            if (connection.State == WebSocketState.Open)
            {
                await connection.SendAsync(data, WebSocketMessageType.Text, true, CancellationToken.None);
            }
        }
    }
}

public class PlayerData
{
    public string Id { get; set; }
    public Vector3 Position { get; set; }
    public int Health { get; set; }
    public List<Item> Inventory { get; set; } = new();
    public List<string> LastActions { get; set; } = new();
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 150
```python
import asyncio
import websockets
import json
from typing import Dict, Set
import time

class PlayerData:
    def __init__(self, player_id: str):
        self.player_id = player_id
        self.position = {"x": 0, "y": 0, "z": 0}
        self.health = 100
        self.inventory = []
        self.game_history = []
        self.last_activity = time.time()

class GameServer:
    def __init__(self):
        self.connected_players: Dict[str, websockets.WebSocketServerProtocol] = {}
        self.player_data: Dict[str, PlayerData] = {}
        self.game_state = {}

    async def register_player(self, websocket, player_id: str):
        # Vulnerable: No connection limit per player or total
        self.connected_players[player_id] = websocket
        self.player_data[player_id] = PlayerData(player_id)

        print(f"Player {player_id} connected. Total players: {len(self.connected_players)}")

    async def handle_player_message(self, websocket, message: str, player_id: str):
        try:
            data = json.loads(message)
            action = data.get('action')

            # Vulnerable: No rate limiting on messages
            if action == 'move':
                await self.handle_movement(player_id, data)
            elif action == 'chat':
                await self.handle_chat(player_id, data)
            elif action == 'inventory':
                await self.handle_inventory(player_id, data)
            elif action == 'attack':
                await self.handle_attack(player_id, data)

        except json.JSONDecodeError:
            pass

    async def handle_movement(self, player_id: str, data: dict):
        if player_id in self.player_data:
            player = self.player_data[player_id]

            # Vulnerable: Storing unlimited movement history
            player.game_history.append({
                'action': 'move',
                'timestamp': time.time(),
                'data': data
            })

            # Update position without validation
            player.position = data.get('position', player.position)

            # Broadcast to all players without filtering
            await self.broadcast_to_all({
                'type': 'player_moved',
                'player_id': player_id,
                'position': player.position
            })

    async def handle_chat(self, player_id: str, data: dict):
        message = data.get('message', '')

        # Vulnerable: No message length or frequency limits
        chat_data = {
            'type': 'chat',
            'player_id': player_id,
            'message': message,
            'timestamp': time.time()
        }

        # Store chat history indefinitely
        if player_id in self.player_data:
            self.player_data[player_id].game_history.append(chat_data)

        # Broadcast to all players
        await self.broadcast_to_all(chat_data)

    async def handle_inventory(self, player_id: str, data: dict):
        if player_id in self.player_data:
            player = self.player_data[player_id]

            # Vulnerable: No inventory size limits
            items = data.get('items', [])
            player.inventory.extend(items)

            # Store inventory changes
            player.game_history.append({
                'action': 'inventory_update',
                'timestamp': time.time(),
                'items_added': len(items)
            })

    async def broadcast_to_all(self, message: dict):
        # Vulnerable: Broadcasting to all connections without limits
        if self.connected_players:
            message_str = json.dumps(message)

            # Send to all connected players
            disconnected = []
            for player_id, websocket in self.connected_players.items():
                try:
                    await websocket.send(message_str)
                except websockets.exceptions.ConnectionClosed:
                    disconnected.append(player_id)

            # Remove disconnected players
            for player_id in disconnected:
                del self.connected_players[player_id]

    async def handle_client(self, websocket, path):
        player_id = f"player_{len(self.connected_players) + 1}"
        await self.register_player(websocket, player_id)

        try:
            async for message in websocket:
                # Vulnerable: No message size or rate limits
                await self.handle_player_message(websocket, message, player_id)
        except websockets.exceptions.ConnectionClosed:
            pass
        finally:
            # Cleanup on disconnect
            if player_id in self.connected_players:
                del self.connected_players[player_id]

# Start game server
async def start_server():
    game_server = GameServer()

    # Vulnerable: No connection limits
    server = await websockets.serve(
        game_server.handle_client,
        "localhost",
        8765,
        max_size=None,  # No message size limit
        max_queue=None  # No queue limit
    )

    print("Game server started on ws://localhost:8765")
    await server.wait_closed()

if __name__ == "__main__":
    asyncio.run(start_server())
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 67
```typescript
import WebSocket from 'ws';

interface PlayerData {
    id: string;
    position: { x: number; y: number; z: number };
    health: number;
    inventory: any[];
    lastActions: string[];
}

class GameServer {
    private players: Map<string, PlayerData> = new Map();
    private connections: Map<string, WebSocket> = new Map();

    handleConnection(ws: WebSocket, playerId: string): void {
        // Vulnerable: No connection limit validation
        this.connections.set(playerId, ws);
        this.players.set(playerId, {
            id: playerId,
            position: { x: 0, y: 0, z: 0 },
            health: 100,
            inventory: [],
            lastActions: []
        });

        ws.on('message', (data: Buffer) => {
            // Vulnerable: No rate limiting or size validation
            this.processGameData(playerId, data.toString());
        });

        ws.on('close', () => {
            // Memory not cleaned up properly
            this.connections.delete(playerId);
        });
    }

    private processGameData(playerId: string, gameAction: string): void {
        // Vulnerable: Unlimited memory growth
        const player = this.players.get(playerId);
        if (player) {
            player.lastActions.push(gameAction);

            // No validation on action data
            if (gameAction.includes('move')) {
                // Process movement without bounds checking
                const coords = gameAction.split(',');
                player.position.x = parseInt(coords[1]) || 0;
                player.position.y = parseInt(coords[2]) || 0;
                player.position.z = parseInt(coords[3]) || 0;
            }
        }

        // Broadcast to all without filtering
        this.broadcastToAll(gameAction);
    }

    private broadcastToAll(message: string): void {
        // Vulnerable: No bandwidth or connection validation
        this.connections.forEach((ws) => {
            if (ws.readyState === WebSocket.OPEN) {
                ws.send(message);
            }
        });
    }
}

const server = new GameServer();
```

### ðŸ§ª Senaryo 8: Bir mail sistemi kullanÄ±cÄ±larÄ±n bÃ¼yÃ¼k ekler gÃ¶ndermesine izin veriyor. E-postalar ek dosyalarÄ±yla birlikte sunucuda saklanÄ±r. KullanÄ±cÄ±lar Ã§eÅŸitli tÃ¼rde dosyalar ekleyebilir. Mail kutularÄ± zaman iÃ§inde bÃ¼yÃ¼r.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 88
```csharp
public class EmailService
{
    private readonly ApplicationDbContext _context;
    private readonly string _attachmentsPath;
    private readonly ILogger<EmailService> _logger;

    public EmailService(ApplicationDbContext context, IConfiguration config, ILogger<EmailService> logger)
    {
        _context = context;
        _attachmentsPath = config["Email:AttachmentsPath"];
        _logger = logger;
        Directory.CreateDirectory(_attachmentsPath);
    }

    public async Task<SendEmailResult> SendEmailAsync(SendEmailRequest request)
    {
        try
        {
            var email = new Email
            {
                Id = Guid.NewGuid(),
                FromUserId = request.FromUserId,
                ToAddresses = string.Join(",", request.ToAddresses),
                Subject = request.Subject,
                Body = request.Body,
                SentAt = DateTime.UtcNow,
                Attachments = new List<EmailAttachment>()
            };

            // Process attachments
            foreach (var attachment in request.Attachments)
            {
                var attachmentInfo = await SaveAttachmentAsync(attachment, email.Id);
                email.Attachments.Add(attachmentInfo);
                email.TotalSize += attachmentInfo.FileSize;
            }

            _context.Emails.Add(email);

            // Add to recipient inboxes
            foreach (var toAddress in request.ToAddresses)
            {
                var recipient = await _context.Users.FirstOrDefaultAsync(u => u.Email == toAddress);
                if (recipient != null)
                {
                    var inbox = new InboxItem
                    {
                        UserId = recipient.Id,
                        EmailId = email.Id,
                        IsRead = false,
                        ReceivedAt = DateTime.UtcNow
                    };
                    _context.InboxItems.Add(inbox);
                }
            }

            await _context.SaveChangesAsync();
            _logger.LogInformation($"Email sent successfully: {email.Id}");

            return new SendEmailResult { Success = true, EmailId = email.Id };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Error sending email");
            return new SendEmailResult { Success = false, Error = ex.Message };
        }
    }

    private async Task<EmailAttachment> SaveAttachmentAsync(IFormFile file, Guid emailId)
    {
        var fileName = $"{emailId}_{Guid.NewGuid()}{Path.GetExtension(file.FileName)}";
        var filePath = Path.Combine(_attachmentsPath, fileName);

        using (var stream = new FileStream(filePath, FileMode.Create))
        {
            await file.CopyToAsync(stream);
        }

        return new EmailAttachment
        {
            OriginalFileName = file.FileName,
            StoredFileName = fileName,
            FilePath = filePath,
            FileSize = file.Length,
            ContentType = file.ContentType
        };
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 198
```python
import os
import uuid
import aiofiles
from dataclasses import dataclass
from typing import List, Optional
from fastapi import UploadFile
from sqlalchemy.orm import Session

@dataclass
class SendEmailRequest:
    from_user_id: int
    to_addresses: List[str]
    subject: str
    body: str
    attachments: List[UploadFile]

@dataclass
class SendEmailResult:
    success: bool
    email_id: Optional[str] = None
    error: Optional[str] = None

class EmailService:
    def __init__(self, db: Session, attachments_path: str):
        self.db = db
        self.attachments_path = attachments_path
        os.makedirs(attachments_path, exist_ok=True)

    async def send_email(self, request: SendEmailRequest) -> SendEmailResult:
        try:
            email_id = str(uuid.uuid4())

            # Create email record
            email = Email(
                id=email_id,
                from_user_id=request.from_user_id,
                to_addresses=",".join(request.to_addresses),
                subject=request.subject,
                body=request.body,
                sent_at=datetime.utcnow(),
                total_size=0
            )

            # Process attachments
            total_size = 0
            for attachment in request.attachments:
                attachment_info = await self.save_attachment(attachment, email_id)

                email_attachment = EmailAttachment(
                    email_id=email_id,
                    original_filename=attachment_info['original_filename'],
                    stored_filename=attachment_info['stored_filename'],
                    file_path=attachment_info['file_path'],
                    file_size=attachment_info['file_size'],
                    content_type=attachment_info['content_type']
                )

                self.db.add(email_attachment)
                total_size += attachment_info['file_size']

            email.total_size = total_size
            self.db.add(email)

            # Add to recipient inboxes
            for to_address in request.to_addresses:
                recipient = self.db.query(User).filter(User.email == to_address).first()
                if recipient:
                    inbox_item = InboxItem(
                        user_id=recipient.id,
                        email_id=email_id,
                        is_read=False,
                        received_at=datetime.utcnow()
                    )
                    self.db.add(inbox_item)

            self.db.commit()
            return SendEmailResult(success=True, email_id=email_id)

        except Exception as e:
            self.db.rollback()
            return SendEmailResult(success=False, error=str(e))

    async def save_attachment(self, file: UploadFile, email_id: str) -> dict:
        file_extension = os.path.splitext(file.filename)[1]
        stored_filename = f"{email_id}_{uuid.uuid4()}{file_extension}"
        file_path = os.path.join(self.attachments_path, stored_filename)

        async with aiofiles.open(file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

```typescript
import { promises as fs } from 'fs';
import path from 'path';
import { v4 as uuidv4 } from 'uuid';
import { EmailRepository } from './repositories/EmailRepository';

interface SendEmailRequest {
    fromUserId: number;
    toAddresses: string[];
    subject: string;
    body: string;
    attachments: Express.Multer.File[];
}

interface EmailAttachment {
    originalFilename: string;
    storedFilename: string;
    filePath: string;
    fileSize: number;
    contentType: string;
}

export class EmailService {
    private emailRepository: EmailRepository;
    private attachmentsPath: string;

    constructor(emailRepository: EmailRepository, attachmentsPath: string) {
        this.emailRepository = emailRepository;
        this.attachmentsPath = attachmentsPath;
        fs.mkdir(attachmentsPath, { recursive: true });
    }

    async sendEmail(request: SendEmailRequest): Promise<{ success: boolean; emailId?: string; error?: string }> {
        try {
            const emailId = uuidv4();
            let totalSize = 0;

            // Process attachments
            const attachments: EmailAttachment[] = [];
            for (const file of request.attachments) {
                const attachment = await this.saveAttachment(file, emailId);
                attachments.push(attachment);
                totalSize += attachment.fileSize;
            }

            // Create email record
            const email = {
                id: emailId,
                fromUserId: request.fromUserId,
                toAddresses: request.toAddresses.join(','),
                subject: request.subject,
                body: request.body,
                sentAt: new Date(),
                totalSize,
                attachments
            };

            await this.emailRepository.create(email);

            // Add to recipient inboxes
            for (const toAddress of request.toAddresses) {
                const recipient = await this.emailRepository.findUserByEmail(toAddress);
                if (recipient) {
                    await this.emailRepository.addToInbox({
                        userId: recipient.id,
                        emailId,
                        isRead: false,
                        receivedAt: new Date()
                    });
                }
            }

            console.log(`Email sent successfully: ${emailId}`);
            return { success: true, emailId };

        } catch (error) {
            console.error('Error sending email:', error);
            return { success: false, error: error.message };
        }
    }

    private async saveAttachment(file: Express.Multer.File, emailId: string): Promise<EmailAttachment> {
        const fileExtension = path.extname(file.originalname);
        const storedFilename = `${emailId}_${uuidv4()}${fileExtension}`;
        const filePath = path.join(this.attachmentsPath, storedFilename);

        await fs.writeFile(filePath, file.buffer);

        return {
            originalFilename: file.originalname,
            storedFilename,
            filePath,
            fileSize: file.size,
            contentType: file.mimetype
        };
    }

    async getMailboxSize(userId: number): Promise<number> {
        return await this.emailRepository.calculateMailboxSize(userId);
    }
}
```

### ðŸ§ª Senaryo 9: Bir raporlama sistemi bÃ¼yÃ¼k veri setleri Ã¼zerinde analiz yapÄ±yor. KullanÄ±cÄ±lar geniÅŸ tarih aralÄ±klarÄ±nda raporlar oluÅŸturabilir. Sistem milyonlarca kayÄ±t Ã¼zerinde hesaplama yapar. OluÅŸturulan raporlar kullanÄ±cÄ±lara sunulur.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 90
```csharp
public class ReportingService
{
    private readonly ApplicationDbContext _context;
    private readonly ILogger<ReportingService> _logger;

    public ReportingService(ApplicationDbContext context, ILogger<ReportingService> logger)
    {
        _context = context;
        _logger = logger;
    }

    public async Task<ReportResult> GenerateReportAsync(ReportRequest request)
    {
        _logger.LogInformation($"Starting report generation: {request.ReportType}");

        var report = new Report
        {
            Id = Guid.NewGuid(),
            UserId = request.UserId,
            ReportType = request.ReportType,
            StartDate = request.StartDate,
            EndDate = request.EndDate,
            Status = ReportStatus.Processing,
            CreatedAt = DateTime.UtcNow
        };

        _context.Reports.Add(report);
        await _context.SaveChangesAsync();

        // Process in background
        _ = Task.Run(() => ProcessReportAsync(report.Id, request));

        return new ReportResult { Success = true, ReportId = report.Id };
    }

    private async Task ProcessReportAsync(Guid reportId, ReportRequest request)
    {
        try
        {
            var report = await _context.Reports.FindAsync(reportId);

            var data = request.ReportType switch
            {
                "sales" => await GenerateSalesReportAsync(request),
                "user_activity" => await GenerateUserActivityReportAsync(request),
                "performance" => await GeneratePerformanceReportAsync(request),
                _ => throw new ArgumentException("Invalid report type")
            };

            report.Data = JsonSerializer.Serialize(data);
            report.RecordCount = data.TotalRecords;
            report.Status = ReportStatus.Completed;
            report.CompletedAt = DateTime.UtcNow;

            await _context.SaveChangesAsync();

            _logger.LogInformation($"Report completed: {reportId}");
        }
        catch (Exception ex)
        {
            var report = await _context.Reports.FindAsync(reportId);
            report.Status = ReportStatus.Failed;
            report.ErrorMessage = ex.Message;
            await _context.SaveChangesAsync();

            _logger.LogError(ex, $"Report failed: {reportId}");
        }
    }

    private async Task<ReportData> GenerateSalesReportAsync(ReportRequest request)
    {
        var salesData = await _context.Sales
            .Where(s => s.Date >= request.StartDate && s.Date <= request.EndDate)
            .GroupBy(s => new { s.Date.Year, s.Date.Month })
            .Select(g => new
            {
                Period = $"{g.Key.Year}-{g.Key.Month:D2}",
                TotalSales = g.Sum(s => s.Amount),
                OrderCount = g.Count()
            })
            .ToListAsync();

        return new ReportData
        {
            TotalRecords = salesData.Count,
            Summary = new { TotalRevenue = salesData.Sum(s => s.TotalSales) },
            Details = salesData
        };
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 103
```python
import asyncio
import json
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, Any, Optional
from sqlalchemy import func, and_
from sqlalchemy.orm import Session

@dataclass
class ReportRequest:
    user_id: int
    report_type: str
    start_date: datetime
    end_date: datetime
    filters: Optional[Dict[str, Any]] = None

class ReportingService:
    def __init__(self, db: Session):
        self.db = db

    async def generate_report(self, request: ReportRequest) -> dict:
        # Create report record
        report = Report(
            id=str(uuid.uuid4()),
            user_id=request.user_id,
            report_type=request.report_type,
            start_date=request.start_date,
            end_date=request.end_date,
            status='processing',
            created_at=datetime.utcnow()
        )

        self.db.add(report)
        self.db.commit()

        # Process asynchronously
        asyncio.create_task(self.process_report(report.id, request))

        return {'success': True, 'report_id': report.id}

    async def process_report(self, report_id: str, request: ReportRequest):
        try:
            report = self.db.query(Report).filter(Report.id == report_id).first()

            # Generate data based on report type
            if request.report_type == 'sales':
                data = await self.generate_sales_report(request)
            elif request.report_type == 'user_activity':
                data = await self.generate_user_activity_report(request)
            elif request.report_type == 'performance':
                data = await self.generate_performance_report(request)
            else:
                raise ValueError("Invalid report type")

            # Update report with results
            report.data = json.dumps(data)
            report.record_count = data['total_records']
            report.status = 'completed'
            report.completed_at = datetime.utcnow()

            self.db.commit()
            print(f"Report completed: {report_id}")

        except Exception as e:
            report = self.db.query(Report).filter(Report.id == report_id).first()
            report.status = 'failed'
            report.error_message = str(e)
            self.db.commit()
            print(f"Report failed: {report_id} - {str(e)}")

    async def generate_sales_report(self, request: ReportRequest) -> dict:
        # Query large dataset with aggregation
        sales_data = self.db.query(
            func.date_trunc('month', Sale.date).label('period'),
            func.sum(Sale.amount).label('total_sales'),
            func.count(Sale.id).label('order_count')
        ).filter(
            and_(Sale.date >= request.start_date, Sale.date <= request.end_date)
        ).group_by(
            func.date_trunc('month', Sale.date)
        ).all()

        # Process results
        results = [
            {
                'period': row.period.strftime('%Y-%m'),
                'total_sales': float(row.total_sales),
                'order_count': row.order_count
            }
            for row in sales_data
        ]

        return {
            'total_records': len(results),
            'summary': {'total_revenue': sum(r['total_sales'] for r in results)},
            'details': results
        }

    async def get_report_status(self, report_id: str) -> dict:
        report = self.db.query(Report).filter(Report.id == report_id).first()
        if not report:
            return {'error': 'Report not found'}

```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 122
```typescript
import { Repository } from 'typeorm';
import { v4 as uuidv4 } from 'uuid';

interface ReportRequest {
    userId: number;
    reportType: string;
    startDate: Date;
    endDate: Date;
    filters?: Record<string, any>;
}

interface ReportData {
    totalRecords: number;
    summary: Record<string, any>;
    details: any[];
}

export class ReportingService {
    constructor(
        private reportRepository: Repository<Report>,
        private salesRepository: Repository<Sale>
    ) {}

    async generateReport(request: ReportRequest): Promise<{ success: boolean; reportId: string }> {
        const reportId = uuidv4();

        const report = this.reportRepository.create({
            id: reportId,
            userId: request.userId,
            reportType: request.reportType,
            startDate: request.startDate,
            endDate: request.endDate,
            status: 'processing',
            createdAt: new Date()
        });

        await this.reportRepository.save(report);

        // Process asynchronously
        this.processReport(reportId, request).catch(console.error);

        return { success: true, reportId };
    }

    private async processReport(reportId: string, request: ReportRequest): Promise<void> {
        try {
            let data: ReportData;

            switch (request.reportType) {
                case 'sales':
                    data = await this.generateSalesReport(request);
                    break;
                case 'user_activity':
                    data = await this.generateUserActivityReport(request);
                    break;
                default:
                    throw new Error('Invalid report type');
            }

            await this.reportRepository.update(reportId, {
                data: JSON.stringify(data),
                recordCount: data.totalRecords,
                status: 'completed',
                completedAt: new Date()
            });

            console.log(`Report completed: ${reportId}`);

        } catch (error) {
            await this.reportRepository.update(reportId, {
                status: 'failed',
                errorMessage: error.message
            });

            console.error(`Report failed: ${reportId}`, error);
        }
    }

    private async generateSalesReport(request: ReportRequest): Promise<ReportData> {
        const salesData = await this.salesRepository
            .createQueryBuilder('sale')
            .select([
                "DATE_TRUNC('month', sale.date) AS period",
                'SUM(sale.amount) AS totalSales',
                'COUNT(sale.id) AS orderCount'
            ])
            .where('sale.date >= :startDate AND sale.date <= :endDate', {
                startDate: request.startDate,
                endDate: request.endDate
            })
            .groupBy("DATE_TRUNC('month', sale.date)")
            .orderBy('period')
            .getRawMany();

        const totalRevenue = salesData.reduce((sum, row) => sum + parseFloat(row.totalsales), 0);

        return {
            totalRecords: salesData.length,
            summary: { totalRevenue },
            details: salesData.map(row => ({
                period: row.period,
                totalSales: parseFloat(row.totalsales),
                orderCount: parseInt(row.ordercount)
            }))
        };
    }

    async getReportStatus(reportId: string): Promise<any> {
        const report = await this.reportRepository.findOne({ where: { id: reportId } });
        if (!report) {
            throw new Error('Report not found');
        }

        return {
            id: report.id,
            status: report.status,
            createdAt: report.createdAt,
            completedAt: report.completedAt,
            recordCount: report.recordCount
        };
    }
}
```

### ðŸ§ª Senaryo 10: Bir backup sistemi kullanÄ±cÄ± verilerini dÃ¼zenli olarak yedekliyor. FarklÄ± boyutlardaki dosyalar yedekleme sÃ¼recine dahil edilir. Sistem tÃ¼m kullanÄ±cÄ± verilerini kopyalar ve arÅŸivler. Yedekleme iÅŸlemi otomatik olarak Ã§alÄ±ÅŸÄ±r.
**ðŸ’» Dil:** `C#`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 122
```csharp
public class BackupService
{
    private readonly ILogger<BackupService> _logger;
    private readonly ApplicationDbContext _context;
    private readonly string _dataPath;
    private readonly string _backupPath;
    private readonly Timer _backupTimer;

    public BackupService(ILogger<BackupService> logger, ApplicationDbContext context, IConfiguration config)
    {
        _logger = logger;
        _context = context;
        _dataPath = config["Backup:DataPath"];
        _backupPath = config["Backup:BackupPath"];

        Directory.CreateDirectory(_backupPath);

        // Schedule daily backups at 2 AM
        var dailyTime = TimeSpan.FromHours(2);
        var timeToGo = dailyTime - DateTime.Now.TimeOfDay;
        if (timeToGo < TimeSpan.Zero)
            timeToGo = timeToGo.Add(TimeSpan.FromDays(1));

        _backupTimer = new Timer(ExecuteBackup, null, timeToGo, TimeSpan.FromDays(1));
    }

    private async void ExecuteBackup(object state)
    {
        var backupId = Guid.NewGuid();
        _logger.LogInformation($"Starting backup: {backupId}");

        try
        {
            var backup = new BackupRecord
            {
                Id = backupId,
                StartedAt = DateTime.UtcNow,
                Status = BackupStatus.InProgress
            };

            _context.BackupRecords.Add(backup);
            await _context.SaveChangesAsync();

            var backupFolder = Path.Combine(_backupPath, $"backup_{DateTime.Now:yyyyMMdd_HHmmss}");
            Directory.CreateDirectory(backupFolder);

            // Backup user files
            var totalSize = await BackupUserFilesAsync(backupFolder);

            // Backup database
            await BackupDatabaseAsync(backupFolder);

            // Create archive
            var archivePath = await CreateArchiveAsync(backupFolder);

            backup.Status = BackupStatus.Completed;
            backup.CompletedAt = DateTime.UtcNow;
            backup.TotalSize = totalSize;
            backup.ArchivePath = archivePath;

            await _context.SaveChangesAsync();

            // Cleanup old backups
            await CleanupOldBackupsAsync();

            _logger.LogInformation($"Backup completed: {backupId}");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, $"Backup failed: {backupId}");

            var backup = await _context.BackupRecords.FindAsync(backupId);
            if (backup != null)
            {
                backup.Status = BackupStatus.Failed;
                backup.ErrorMessage = ex.Message;
                await _context.SaveChangesAsync();
            }
        }
    }

    private async Task<long> BackupUserFilesAsync(string backupFolder)
    {
        var userFilesPath = Path.Combine(backupFolder, "user_files");
        Directory.CreateDirectory(userFilesPath);

        var users = await _context.Users.ToListAsync();
        long totalSize = 0;

        foreach (var user in users)
        {
            var userFolder = Path.Combine(_dataPath, "users", user.Id.ToString());
            if (Directory.Exists(userFolder))
            {
                var userBackupFolder = Path.Combine(userFilesPath, user.Id.ToString());
                Directory.CreateDirectory(userBackupFolder);

                totalSize += await CopyDirectoryAsync(userFolder, userBackupFolder);
            }
        }

        return totalSize;
    }

    private async Task<long> CopyDirectoryAsync(string sourceDir, string targetDir)
    {
        long totalSize = 0;

        foreach (var file in Directory.GetFiles(sourceDir, "*", SearchOption.AllDirectories))
        {
            var relativePath = Path.GetRelativePath(sourceDir, file);
            var targetFile = Path.Combine(targetDir, relativePath);

            Directory.CreateDirectory(Path.GetDirectoryName(targetFile));
            await Task.Run(() => File.Copy(file, targetFile, true));

            totalSize += new FileInfo(file).Length;
        }

        return totalSize;
    }
}
```

**ðŸ’» Dil:** `Python`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 133
```python
import os
import shutil
import asyncio
import zipfile
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import Optional
import schedule
import threading

@dataclass
class BackupRecord:
    id: str
    started_at: datetime
    completed_at: Optional[datetime] = None
    status: str = "in_progress"
    total_size: int = 0
    archive_path: Optional[str] = None
    error_message: Optional[str] = None

class BackupService:
    def __init__(self, db_session, data_path: str, backup_path: str):
        self.db = db_session
        self.data_path = data_path
        self.backup_path = backup_path

        os.makedirs(backup_path, exist_ok=True)

        # Schedule daily backups at 2 AM
        schedule.every().day.at("02:00").do(self.execute_backup)

        # Start scheduler in background thread
        scheduler_thread = threading.Thread(target=self.run_scheduler, daemon=True)
        scheduler_thread.start()

    def run_scheduler(self):
        while True:
            schedule.run_pending()
            time.sleep(60)

    async def execute_backup(self):
        backup_id = str(uuid.uuid4())
        print(f"Starting backup: {backup_id}")

        try:
            backup_record = BackupRecord(
                id=backup_id,
                started_at=datetime.utcnow()
            )

            # Create backup directory
            backup_folder = os.path.join(
                self.backup_path,
                f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            os.makedirs(backup_folder)

            # Backup user files
            total_size = await self.backup_user_files(backup_folder)

            # Backup database
            await self.backup_database(backup_folder)

            # Create archive
            archive_path = await self.create_archive(backup_folder)

            backup_record.status = "completed"
            backup_record.completed_at = datetime.utcnow()
            backup_record.total_size = total_size
            backup_record.archive_path = archive_path

            # Save to database
            self.save_backup_record(backup_record)

            # Cleanup old backups
            await self.cleanup_old_backups()

            print(f"Backup completed: {backup_id}")

        except Exception as e:
            print(f"Backup failed: {backup_id} - {str(e)}")
            backup_record.status = "failed"
            backup_record.error_message = str(e)
            self.save_backup_record(backup_record)

    async def backup_user_files(self, backup_folder: str) -> int:
        user_files_path = os.path.join(backup_folder, "user_files")
        os.makedirs(user_files_path)

        users = self.db.query(User).all()
        total_size = 0

        for user in users:
            user_folder = os.path.join(self.data_path, "users", str(user.id))
            if os.path.exists(user_folder):
                user_backup_folder = os.path.join(user_files_path, str(user.id))
                total_size += await self.copy_directory(user_folder, user_backup_folder)

        return total_size

    async def copy_directory(self, source_dir: str, target_dir: str) -> int:
        total_size = 0

        for root, dirs, files in os.walk(source_dir):
            for file in files:
                source_file = os.path.join(root, file)
                relative_path = os.path.relpath(source_file, source_dir)
                target_file = os.path.join(target_dir, relative_path)

                os.makedirs(os.path.dirname(target_file), exist_ok=True)
                await asyncio.to_thread(shutil.copy2, source_file, target_file)

                total_size += os.path.getsize(source_file)

        return total_size

    async def create_archive(self, backup_folder: str) -> str:
        archive_path = f"{backup_folder}.zip"

        def create_zip():
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(backup_folder):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, backup_folder)
                        zipf.write(file_path, arcname)

        await asyncio.to_thread(create_zip)

        # Remove uncompressed folder
        shutil.rmtree(backup_folder)

        return archive_path
```

**ðŸ’» Dil:** `TypeScript`
**ðŸ¤– AI:** Claude

**SatÄ±r SayÄ±sÄ±:** 234
```typescript
import * as fs from 'fs';
import * as path from 'path';
import * as archiver from 'archiver';
import { EventEmitter } from 'events';

interface BackupJob {
    userId: string;
    sourcePaths: string[];
    backupType: 'full' | 'incremental';
    scheduledTime: Date;
    priority: number;
}

interface UserData {
    userId: string;
    totalSize: number;
    fileCount: number;
    lastBackup: Date;
}

class BackupSystem extends EventEmitter {
    private backupQueue: BackupJob[] = [];
    private activeBackups: Map<string, BackupJob> = new Map();
    private userDataMap: Map<string, UserData> = new Map();
    private backupStorage: string = '/backup/storage';

    constructor() {
        super();
        this.startBackupScheduler();
    }

    scheduleBackup(job: BackupJob): void {
        // Vulnerable: No limit on backup queue size
        this.backupQueue.push(job);
        console.log(`Backup scheduled for user ${job.userId}. Queue size: ${this.backupQueue.length}`);

        // Process immediately if high priority
        if (job.priority > 8) {
            this.processBackupQueue();
        }
    }

    async processBackupQueue(): Promise<void> {
        // Vulnerable: Processing all queued backups without resource limits
        while (this.backupQueue.length > 0) {
            const job = this.backupQueue.shift()!;

            // Start backup without checking system resources
            this.startBackup(job);
        }
    }

    private async startBackup(job: BackupJob): Promise<void> {
        this.activeBackups.set(job.userId, job);

        try {
            // Vulnerable: No concurrent backup limits
            await this.performBackup(job);
        } catch (error) {
            console.error(`Backup failed for user ${job.userId}:`, error);
        } finally {
            this.activeBackups.delete(job.userId);
        }
    }

    private async performBackup(job: BackupJob): Promise<void> {
        const backupPath = path.join(this.backupStorage, job.userId, new Date().toISOString());

        // Create backup directory
        await fs.promises.mkdir(backupPath, { recursive: true });

        let totalSize = 0;
        let fileCount = 0;

        // Vulnerable: Processing all files without size or count limits
        for (const sourcePath of job.sourcePaths) {
            const stats = await this.processDirectory(sourcePath, backupPath);
            totalSize += stats.size;
            fileCount += stats.count;
        }

        // Create archive without compression limits
        await this.createArchive(backupPath, job.userId);

        // Update user data without limits
        this.updateUserData(job.userId, totalSize, fileCount);

        console.log(`Backup completed for ${job.userId}: ${fileCount} files, ${totalSize} bytes`);
    }

    private async processDirectory(sourcePath: string, backupPath: string): Promise<{size: number, count: number}> {
        let totalSize = 0;
        let fileCount = 0;

        try {
            const entries = await fs.promises.readdir(sourcePath, { withFileTypes: true });

            // Vulnerable: No recursion depth limits
            for (const entry of entries) {
                const fullPath = path.join(sourcePath, entry.name);
                const destPath = path.join(backupPath, entry.name);

                if (entry.isDirectory()) {
                    await fs.promises.mkdir(destPath, { recursive: true });
                    const stats = await this.processDirectory(fullPath, destPath);
                    totalSize += stats.size;
                    fileCount += stats.count;
                } else {
                    // Vulnerable: No file size limits
                    const stats = await fs.promises.stat(fullPath);
                    await fs.promises.copyFile(fullPath, destPath);

                    totalSize += stats.size;
                    fileCount++;
                }
            }
        } catch (error) {
            console.error(`Error processing directory ${sourcePath}:`, error);
        }

        return { size: totalSize, count: fileCount };
    }

    private async createArchive(backupPath: string, userId: string): Promise<void> {
        return new Promise((resolve, reject) => {
            const archivePath = `${backupPath}.zip`;
            const output = fs.createWriteStream(archivePath);
            const archive = archiver('zip', {
                zlib: { level: 9 } // Maximum compression
            });

            output.on('close', () => {
                console.log(`Archive created: ${archive.pointer()} bytes`);
                resolve();
            });

            archive.on('error', reject);
            archive.pipe(output);

            // Vulnerable: Archiving without size limits
            archive.directory(backupPath, false);
            archive.finalize();
        });
    }

    private updateUserData(userId: string, totalSize: number, fileCount: number): void {
        const existing = this.userDataMap.get(userId) || {
            userId,
            totalSize: 0,
            fileCount: 0,
            lastBackup: new Date()
        };

        // Vulnerable: Accumulating data without limits
        existing.totalSize += totalSize;
        existing.fileCount += fileCount;
        existing.lastBackup = new Date();

        this.userDataMap.set(userId, existing);
    }

    private startBackupScheduler(): void {
        // Vulnerable: Running backup scheduler without throttling
        setInterval(() => {
            this.processBackupQueue();
        }, 1000); // Process every second

        // Vulnerable: Auto-scheduling backups for all users
        setInterval(() => {
            this.scheduleAllUserBackups();
        }, 60000); // Schedule every minute
    }

    private scheduleAllUserBackups(): void {
        // Vulnerable: Creating backup jobs for all users simultaneously
        const allUsers = Array.from(this.userDataMap.keys());

        for (const userId of allUsers) {
            const job: BackupJob = {
                userId,
                sourcePaths: [`/users/${userId}/documents`, `/users/${userId}/photos`],
                backupType: 'incremental',
                scheduledTime: new Date(),
                priority: 5
            };

            this.scheduleBackup(job);
        }
    }

    getSystemStatus(): any {
        return {
            queueSize: this.backupQueue.length,
            activeBackups: this.activeBackups.size,
            totalUsers: this.userDataMap.size,
            totalStorageUsed: Array.from(this.userDataMap.values())
                .reduce((sum, user) => sum + user.totalSize, 0)
        };
    }

    // Vulnerable: Public method to trigger mass backups
    triggerEmergencyBackup(): void {
        const allUsers = ['user1', 'user2', 'user3', 'user4', 'user5']; // Simulate many users

        for (const userId of allUsers) {
            const job: BackupJob = {
                userId,
                sourcePaths: [`/users/${userId}`],
                backupType: 'full',
                scheduledTime: new Date(),
                priority: 10
            };

            this.scheduleBackup(job);
        }
    }
}

// Usage
const backupSystem = new BackupSystem();

// Simulate normal backup scheduling
const normalJob: BackupJob = {
    userId: 'user123',
    sourcePaths: ['/home/user123/documents', '/home/user123/photos'],
    backupType: 'full',
    scheduledTime: new Date(),
    priority: 5
};

backupSystem.scheduleBackup(normalJob);

// Vulnerable: Allow external trigger of resource-intensive operations
export { BackupSystem, BackupJob };
```